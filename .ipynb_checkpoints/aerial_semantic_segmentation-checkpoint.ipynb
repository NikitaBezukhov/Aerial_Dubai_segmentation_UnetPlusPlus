{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ep-_A2-tWw7Y",
    "outputId": "ebf0bac7-eba4-4c99-dce6-25fadc554885"
   },
   "source": [
    "# __This is a notebook of a PyTorch Dubai Aerial photo segmentation with Pytorch Unet++ (ResNet101 backbone)!__ \n",
    "# Project by __[Nikita Bezukhov](https://github.com/NikitaBezukhov)__!\n",
    "### The data set is __[Semantic segmentation of aerial imagery from Kaggle](https://www.kaggle.com/humansintheloop/semantic-segmentation-of-aerial-imagery)__.\n",
    "### Was built in __[Google Colab](https://colab.research.google.com/)__ environment, so make any adjustments needed for it to work on your machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "####\n",
    "####\n",
    "## __1. First lets install, import and define all necessary libraries/classes/functions that we will be using.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "outputs": [],
   "source": [
    "pip install barbar git+https://github.com/albumentations-team/albumentations segmentation_models_pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndXCKoSpXx0d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import segmentation_models_pytorch \n",
    "import logging\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "from math import exp\n",
    "import cv2\n",
    "import random\n",
    "import torchvision\n",
    "from imageio import imread\n",
    "import torch.nn as nn\n",
    "from PIL import Image, ImageOps, ImageShow\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from sklearn.metrics import jaccard_score\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, datasets \n",
    "from shutil import copyfile, move\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from barbar import Bar\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import math\n",
    "from torchvision import models\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from itertools import chain\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDlouE6xX9o0"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print, best_score=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = best_score\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when monitored metric decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Monitored metric has improved ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), f'/content/drive/MyDrive/aerialmodel.pt') \n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYm5glYHljG9"
   },
   "outputs": [],
   "source": [
    "def square_the_image(image):\n",
    "  \n",
    "    desired_size = max(image.size[0], image.size[1])\n",
    "    old_size = (image.size)  \n",
    "\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    image = image.resize(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    squared_image = Image.new(\"RGB\", (desired_size, desired_size))\n",
    "    squared_image.paste(image, ((desired_size-new_size[0])//2,\n",
    "                      (desired_size-new_size[1])//2))\n",
    "\n",
    "    return np.array(squared_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sk0273iollaN"
   },
   "outputs": [],
   "source": [
    "color_dict = {0: (0, 0, 0),\n",
    "              1: (254, 221,  58),\n",
    "              2: (132,  41, 246),\n",
    "              3: (110, 193, 228),\n",
    "              4: (60, 16, 152),\n",
    "              5: (226, 169, 41),\n",
    "              6: (155, 155, 155)}\n",
    "\n",
    "\n",
    "def rgb_to_onehot(rgb_arr, color_dict=color_dict):\n",
    "    num_classes = len(color_dict)\n",
    "    shape = rgb_arr.shape[:2]+(num_classes,)\n",
    "    arr = np.zeros( shape, dtype=np.int8 )\n",
    "    for i, cls in enumerate(color_dict):\n",
    "        arr[:,:,i] = np.all(rgb_arr.reshape( (-1,3) ) == color_dict[i], axis=1).reshape(shape[:2])\n",
    "    return torchvision.transforms.ToTensor()(arr)\n",
    "\n",
    "\n",
    "def onehot_to_rgb(onehot, color_dict=color_dict):\n",
    "    single_layer = np.argmax(onehot, axis=-1)\n",
    "    output = np.zeros( onehot.shape[:2]+(3,) )\n",
    "    for k in color_dict.keys():\n",
    "        output[single_layer==k] = color_dict[k]\n",
    "    return np.uint8(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FElQr8MTO-iA"
   },
   "outputs": [],
   "source": [
    "class Metric(object):\n",
    "    \"\"\"Base class for all metrics.\n",
    "    From: https://github.com/pytorch/tnt/blob/master/torchnet/meter/meter.py\n",
    "    \"\"\"\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def add(self):\n",
    "        pass\n",
    "\n",
    "    def value(self):\n",
    "        pass\n",
    "\n",
    "class ConfusionMatrix(Metric):\n",
    "    \"\"\"Constructs a confusion matrix for a multi-class classification problems.\n",
    "    Does not support multi-label, multi-class problems.\n",
    "    Keyword arguments:\n",
    "    - num_classes (int): number of classes in the classification problem.\n",
    "    - normalized (boolean, optional): Determines whether or not the confusion\n",
    "    matrix is normalized or not. Default: False.\n",
    "    Modified from: https://github.com/pytorch/tnt/blob/master/torchnet/meter/confusionmeter.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, normalized=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conf = np.ndarray((num_classes, num_classes), dtype=np.int64)\n",
    "        self.normalized = normalized\n",
    "        self.num_classes = num_classes\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.conf.fill(0)\n",
    "\n",
    "    def add(self, predicted, target):\n",
    "        \"\"\"Computes the confusion matrix\n",
    "        The shape of the confusion matrix is K x K, where K is the number\n",
    "        of classes.\n",
    "        Keyword arguments:\n",
    "        - predicted (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n",
    "        predicted scores obtained from the model for N examples and K classes,\n",
    "        or an N-tensor/array of integer values between 0 and K-1.\n",
    "        - target (Tensor or numpy.ndarray): Can be an N x K tensor/array of\n",
    "        ground-truth classes for N examples and K classes, or an N-tensor/array\n",
    "        of integer values between 0 and K-1.\n",
    "        \"\"\"\n",
    "        # If target and/or predicted are tensors, convert them to numpy arrays\n",
    "        if torch.is_tensor(predicted):\n",
    "            predicted = predicted.cpu().numpy()\n",
    "        if torch.is_tensor(target):\n",
    "            target = target.cpu().numpy()\n",
    "\n",
    "        assert predicted.shape[0] == target.shape[0], \\\n",
    "            'number of targets and predicted outputs do not match'\n",
    "\n",
    "        if np.ndim(predicted) != 1:\n",
    "            assert predicted.shape[1] == self.num_classes, \\\n",
    "                'number of predictions does not match size of confusion matrix'\n",
    "            predicted = np.argmax(predicted, 1)\n",
    "        else:\n",
    "            assert (predicted.max() < self.num_classes) and (predicted.min() >= 0), \\\n",
    "                'predicted values are not between 0 and k-1'\n",
    "\n",
    "        if np.ndim(target) != 1:\n",
    "            assert target.shape[1] == self.num_classes, \\\n",
    "                'Onehot target does not match size of confusion matrix'\n",
    "            assert (target >= 0).all() and (target <= 1).all(), \\\n",
    "                'in one-hot encoding, target values should be 0 or 1'\n",
    "            assert (target.sum(1) == 1).all(), \\\n",
    "                'multi-label setting is not supported'\n",
    "            target = np.argmax(target, 1)\n",
    "        else:\n",
    "            assert (target.max() < self.num_classes) and (target.min() >= 0), \\\n",
    "                'target values are not between 0 and k-1'\n",
    "\n",
    "        # hack for bincounting 2 arrays together\n",
    "        x = predicted + self.num_classes * target\n",
    "        bincount_2d = np.bincount(\n",
    "            x.astype(np.int64), minlength=self.num_classes**2)\n",
    "        assert bincount_2d.size == self.num_classes**2\n",
    "        conf = bincount_2d.reshape((self.num_classes, self.num_classes))\n",
    "\n",
    "        self.conf += conf\n",
    "\n",
    "    def value(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Confustion matrix of K rows and K columns, where rows corresponds\n",
    "            to ground-truth targets and columns corresponds to predicted\n",
    "            targets.\n",
    "        \"\"\"\n",
    "        if self.normalized:\n",
    "            conf = self.conf.astype(np.float32)\n",
    "            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n",
    "        else:\n",
    "            return self.conf        \n",
    "\n",
    "\n",
    "class IoU(Metric):\n",
    "    \"\"\"Computes the intersection over union (IoU) per class and corresponding\n",
    "    mean (mIoU).\n",
    "    Intersection over union (IoU) is a common evaluation metric for semantic\n",
    "    segmentation. The predictions are first accumulated in a confusion matrix\n",
    "    and the IoU is computed from it as follows:\n",
    "        IoU = true_positive / (true_positive + false_positive + false_negative).\n",
    "    Keyword arguments:\n",
    "    - num_classes (int): number of classes in the classification problem\n",
    "    - normalized (boolean, optional): Determines whether or not the confusion\n",
    "    matrix is normalized or not. Default: False.\n",
    "    - ignore_index (int or iterable, optional): Index of the classes to ignore\n",
    "    when computing the IoU. Can be an int, or any iterable of ints.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, normalized=False, ignore_index=None):\n",
    "        super().__init__()\n",
    "        self.conf_metric = ConfusionMatrix(num_classes, normalized)\n",
    "\n",
    "        if ignore_index is None:\n",
    "            self.ignore_index = None\n",
    "        elif isinstance(ignore_index, int):\n",
    "            self.ignore_index = (ignore_index,)\n",
    "        else:\n",
    "            try:\n",
    "                self.ignore_index = tuple(ignore_index)\n",
    "            except TypeError:\n",
    "                raise ValueError(\"'ignore_index' must be an int or iterable\")\n",
    "\n",
    "    def reset(self):\n",
    "        self.conf_metric.reset()\n",
    "\n",
    "    def add(self, predicted, target):\n",
    "        \"\"\"Adds the predicted and target pair to the IoU metric.\n",
    "        Keyword arguments:\n",
    "        - predicted (Tensor): Can be a (N, K, H, W) tensor of\n",
    "        predicted scores obtained from the model for N examples and K classes,\n",
    "        or (N, H, W) tensor of integer values between 0 and K-1.\n",
    "        - target (Tensor): Can be a (N, K, H, W) tensor of\n",
    "        target scores for N examples and K classes, or (N, H, W) tensor of\n",
    "        integer values between 0 and K-1.\n",
    "        \"\"\"\n",
    "        # Dimensions check\n",
    "        assert predicted.size(0) == target.size(0), \\\n",
    "            'number of targets and predicted outputs do not match'\n",
    "        assert predicted.dim() == 3 or predicted.dim() == 4, \\\n",
    "            \"predictions must be of dimension (N, H, W) or (N, K, H, W)\"\n",
    "        assert target.dim() == 3 or target.dim() == 4, \\\n",
    "            \"targets must be of dimension (N, H, W) or (N, K, H, W)\"\n",
    "\n",
    "        # If the tensor is in categorical format convert it to integer format\n",
    "        if predicted.dim() == 4:\n",
    "            _, predicted = predicted.max(1)\n",
    "        if target.dim() == 4:\n",
    "            _, target = target.max(1)\n",
    "\n",
    "        self.conf_metric.add(predicted.view(-1), target.view(-1))\n",
    "\n",
    "    def value(self):\n",
    "        \"\"\"Computes the IoU and mean IoU.\n",
    "        The mean computation ignores NaN elements of the IoU array.\n",
    "        Returns:\n",
    "            Tuple: (IoU, mIoU). The first output is the per class IoU,\n",
    "            for K classes it's numpy.ndarray with K elements. The second output,\n",
    "            is the mean IoU.\n",
    "        \"\"\"\n",
    "        conf_matrix = self.conf_metric.value()\n",
    "        if self.ignore_index is not None:\n",
    "            conf_matrix[:, self.ignore_index] = 0\n",
    "            conf_matrix[self.ignore_index, :] = 0\n",
    "        true_positive = np.diag(conf_matrix)\n",
    "        false_positive = np.sum(conf_matrix, 0) - true_positive\n",
    "        false_negative = np.sum(conf_matrix, 1) - true_positive\n",
    "\n",
    "        # Just in case we get a division by 0, ignore/hide the error\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            iou = true_positive / (true_positive + false_positive + false_negative)\n",
    "\n",
    "        return iou, np.nanmean(iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "## __2. Data preparation.__\n",
    "### Dataset consists of 72 Dubai aerial photos and ground truth masks consisting of 6 classes:\n",
    "#### 1.Building\n",
    "#### 2.Land (unpaved area)\n",
    "#### 3.Road\n",
    "#### 4.Vegetation \n",
    "#### 5.Water \n",
    "#### 6.Unlabeled \n",
    "#### We will one-hot encode our masks for a loss function backprop and then will be able to reverse one-hot to look at results of our model's work with functions we defined above: \"rgb_to_onehot\" and \"onehot_to_rgb\". (All the colors for classes are saved in color_dict of a function.\n",
    "### Since dataset is so small I decided to hand pick 16 different pics for validation and leave other 56 for training.\n",
    "### We will unzip it and put image and mask path in lists for our custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUQeX-6HX_Vd",
    "outputId": "a990bd84-5e8c-4644-a2a7-f9eda54b179b"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/aerial.zip -d /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RlIYwFSX_Tf"
   },
   "outputs": [],
   "source": [
    "train_photos = '/content/train_photo/'\n",
    "train_masks = '/content/train_mask/'\n",
    "val_photos = '/content/val_photo/'\n",
    "val_masks = '/content/val_mask/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzpKToDRX_RQ",
    "outputId": "583490d6-1e3f-476a-eed3-ff6eeaaede76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "56\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(train_photos)))\n",
    "print(len(os.listdir(train_masks)))\n",
    "print(len(os.listdir(val_photos)))\n",
    "print(len(os.listdir(val_masks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JBYtMVyrX_O2"
   },
   "outputs": [],
   "source": [
    "train_photos_paths = [train_photos + photo for photo in os.listdir(train_photos)]\n",
    "train_masks_paths = [train_masks + mask for mask in os.listdir(train_masks)]\n",
    "val_photos_paths = [val_photos + photo for photo in os.listdir(val_photos)]\n",
    "val_masks_paths = [val_masks + mask for mask in os.listdir(val_masks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4kxGZjSaRFn"
   },
   "outputs": [],
   "source": [
    "train_photos_paths = sorted(train_photos_paths)\n",
    "train_masks_paths = sorted(train_masks_paths)\n",
    "val_photos_paths = sorted(val_photos_paths)\n",
    "val_masks_paths = sorted(val_masks_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wm8MmA_aX_Ms",
    "outputId": "121e8c32-e23a-4cc4-a696-90162c4facb6"
   },
   "outputs": [],
   "source": [
    "for i,every in enumerate(train_photos_paths):\n",
    "    print(every,train_masks_paths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uUmTYbZX_Hm",
    "outputId": "b384170b-9d8f-4ee4-8060-d52fd3277551"
   },
   "outputs": [],
   "source": [
    "for i,every in enumerate(val_photos_paths):\n",
    "    print(every,val_masks_paths[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "##\n",
    "## __3. Data augmentation.__\n",
    "### Dataset consists of pictures of different size 500-800 pixels on HxW and are rectangular (but almost square). At the start of the notebook we defined \"square_the_image\" function that will 0 pad each picture so that it becomes a square.\n",
    "### Since pictures have a lot of small detailes, to not lose them I decided to resize all of them to 800x800 pixels. \n",
    "### We will be using Albumentations image augmentation library to transform scans and corresponding masks in the same way.\n",
    "### I found next augmentations to make the most sense for this dataset: affine transformations with zoom 0.9-1.1, shift -0.15-0.15 and rotation -180-180 degrees and shear -10-10 degrees. Also I found random grid shuffle to be an appropriate augmentation for this kind of data. (splitting image into 4 equal squares and randomly shuffling them)\n",
    "### We also incorporate random horizontal flip and normalization. (we will calculate mean and std for normalization later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "mW5hObeYX_FH"
   },
   "outputs": [],
   "source": [
    "image_size = 800\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize (image_size, image_size, p=1.0),\n",
    "    A.RandomGridShuffle(grid=(2, 2), p=0.33),\n",
    "    A.augmentations.geometric.transforms.Affine(scale=(0.9,1.1), translate_percent={'x':(-0.15,0.15),'y':(-0.15,0.15)}, \n",
    "                                                rotate=(-180,180), shear=(-10,10), p=1.0),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "normalize_transforms = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.3630, 0.3662, 0.3691], std=[0.3343, 0.3351, 0.3461])\n",
    "])\n",
    "\n",
    "\n",
    "to_tensor_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize (image_size, image_size, p=1.0),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "### Next we make our custom dataset and specify order of all needed transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2IDjznCX_C7"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_paths, target_paths, train=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.target_paths = target_paths\n",
    "        self.train = train\n",
    "\n",
    "    def transform(self, image, mask, train):\n",
    "        if train:\n",
    "            transformed = train_transforms(image=image, mask=mask)            \n",
    "            tensor_image = to_tensor_transforms(transformed['image'])\n",
    "            normalized_img = normalize_transforms(tensor_image)\n",
    "        else:          \n",
    "            transformed = val_transforms(image=image, mask=mask)\n",
    "            tensor_image = to_tensor_transforms(transformed['image'])\n",
    "            normalized_img = normalize_transforms(tensor_image)\n",
    "            \n",
    "        return normalized_img, transformed['mask']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_paths[index])\n",
    "        mask = Image.open(self.target_paths[index])\n",
    "        squared_image = square_the_image(image)\n",
    "        squared_mask = square_the_image(mask)\n",
    "        x, y = self.transform(squared_image, squared_mask, self.train)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gn8CX-2OX_Bx"
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_photos_paths, train_masks_paths, train=True)\n",
    "val_dataset = MyDataset(val_photos_paths, val_masks_paths, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "### Batch size chosen as 2 because it is all I could fit in 16gb Colab Pro GPU, but we will be using PyTorch gradient accumulation, so effectively we will have batch size of 28-56 images. I trained model on batch size of 28 and then fine-tuned with batch size of 56 to gain last bits of performance. (Random augmentations will prevent us from overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qz_ps-2X-_r"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=2, shuffle=True,\n",
    "    pin_memory=False, drop_last=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=2, shuffle=False,\n",
    "    pin_memory=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "### Mean and std calculation for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HjAn5axwUrmk",
    "outputId": "f92de537-b6a7-47b9-931d-7be6182da5b4"
   },
   "outputs": [],
   "source": [
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "for inputs, label in train_loader:\n",
    "    psum    += inputs.sum(axis        = [0, 2, 3])\n",
    "    psum_sq += (inputs ** 2).sum(axis = [0, 2, 3])\n",
    "# pixel count\n",
    "count = 56 * image_size * image_size\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: '  + str(total_mean))\n",
    "print('std:  '  + str(total_std))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "### We will be using Dice score and IoU as our metrics to measure model performance and Dice as a loss function.\n",
    "### We divide loss by 14-28 because of usage of gradient accumulation (updating weights after every 14-28 minibatch loops). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMZvmc-eGJ02"
   },
   "outputs": [],
   "source": [
    "def Dice_and_iou(inputs, targets, smooth=1e-6):\n",
    "    \n",
    "    #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "    # inputs = F.sigmoid(inputs)       \n",
    "    \n",
    "    #flatten label and prediction tensors\n",
    "    inputs = inputs.reshape(-1)\n",
    "    targets = targets.reshape(-1)\n",
    "    \n",
    "    intersection = (inputs * targets).sum()                            \n",
    "    dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "    iou = (intersection + smooth)/(inputs.sum() + targets.sum() - intersection + smooth)  \n",
    "    return dice, iou    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnTFVlDgX-9x"
   },
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1e-6):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        # inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()  \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)                               \n",
    "        return (1-dice)/28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "## __4. Training.__\n",
    "### Model we will be using is Unet++ with ResNet101 backbone and ImageNet weights from __[here](https://github.com/qubvel/segmentation_models.pytorch)__!\n",
    "### We define it with 7 channel output for a mask. (6 dataset classes + 0 padded black borders from data augmentations)\n",
    "### Model was learning pretty fast so low starting learning rate was chosen to prevent instability. Also because of models fast learning a lot of the times momentum was starting to carry the model away from point of minimum, and since each batch size is big enough and pretty representative of distribution we are trying to learn, I decided to lower momentum to 0.5 for faster and more stable training. ReduceLROnPlateau is also used.\n",
    "### Training process will be logged to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83,
     "referenced_widgets": [
      "18db1510dc21478abad9bd583e2ded1a",
      "d2d54ab1f8c64025b28ce3f533b078d8",
      "f183a0445b264176a8940514bf4e7d9f",
      "5b687de4133e45709d2c0d5a68a6b33a",
      "a8013a5a38714a3a8eca27a7f182977a",
      "95696ed9c6ba4ee09ae774a9fd63236d",
      "770872b820d24bc5992beff5d0745704",
      "32611ed1c5ce4c30929bb5566e86fae6"
     ]
    },
    "id": "MVbNn7eHX-62",
    "outputId": "ed98e6d8-0e08-439f-e532-86741838fb5a"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "num_classes = 7\n",
    "         \n",
    "\n",
    "model = segmentation_models_pytorch.UnetPlusPlus(encoder_name=\"resnet101\",in_channels=3, classes=7,\n",
    "                                                 aux_params=None, encoder_weights=\"imagenet\").to(device)   \n",
    "\n",
    "criterion = DiceLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=0.0001, betas=(0.5,0.999))\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=25, cooldown=10, verbose=True)\n",
    "\n",
    "epochs = 999\n",
    "\n",
    "writer = SummaryWriter(log_dir='/content/drive/MyDrive/aerial_logs/', filename_suffix=\"aerial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "### Patience of 100 epochs was chosen since each epoch is basically 1-2 parameter updates and so big value for patience is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IN2y_uoXjhqn",
    "outputId": "120df1fc-6b7c-4433-d88f-3c9553846946"
   },
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=100, verbose=True)\n",
    "batch_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    all_train_iou = []\n",
    "    all_val_iou = []\n",
    "    all_train_dice = []\n",
    "    all_val_dice = []\n",
    "    print(f'Epoch {epoch+1}')\n",
    "\n",
    "    # Training loop\n",
    "    for idx, (inputs, masks) in enumerate(Bar(train_loader)):\n",
    "        batch_count += 1\n",
    "        model.train()\n",
    "        masks_one_hot = torch.tensor([])\n",
    "        for every in masks:\n",
    "            masks_one_hot = torch.cat((masks_one_hot, rgb_to_onehot(every.numpy()).view(1,num_classes, image_size, image_size)))\n",
    "        inputs, masks_one_hot = inputs.to(device), masks_one_hot.to(device)\n",
    "        outputs = model(inputs) \n",
    "        loss = criterion(F.softmax(outputs,dim=1), masks_one_hot) \n",
    "        loss.backward() \n",
    "        if batch_count % 28 == 0:\n",
    "            optimizer.step() \n",
    "            optimizer.zero_grad() \n",
    "        train_loss.append(28*loss.item())\n",
    "        train_dice, train_iou = Dice_and_iou(F.softmax(outputs,dim=1).round(), masks_one_hot)\n",
    "        all_train_iou.append(train_iou) \n",
    "        all_train_dice.append(train_dice)\n",
    "          \n",
    "    all_train_iou = sum(all_train_iou)/(len(all_train_iou)+1e-6)\n",
    "    all_train_dice = sum(all_train_dice)/(len(all_train_dice)+1e-6)     \n",
    "    print(f\"Train IoU: {all_train_iou}\")\n",
    "    print(f\"Train Dice: {all_train_dice}\")\n",
    "    train_loss_final = sum(train_loss)/(len(train_loss)+1e-6)\n",
    "    scheduler.step(train_loss_final)\n",
    "    train_loss_formated = \"{:.4f}\".format(train_loss_final)\n",
    "\n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        for inputs, masks in val_loader:\n",
    "            model.eval()       \n",
    "            masks_one_hot = torch.tensor([])\n",
    "            for every in masks:\n",
    "                masks_one_hot = torch.cat((masks_one_hot, rgb_to_onehot(every.numpy()).view(1,num_classes, image_size, image_size)))\n",
    "            inputs, masks_one_hot = inputs.to(device), masks_one_hot.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(F.softmax(outputs,dim=1), masks_one_hot) \n",
    "            val_loss.append(28*loss.item())\n",
    "            val_dice, val_iou = Dice_and_iou(F.softmax(outputs,dim=1).round(), masks_one_hot)\n",
    "            all_val_iou.append(val_iou)\n",
    "            all_val_dice.append(val_dice)\n",
    "    \n",
    "    all_val_iou = sum(all_val_iou)/(len(all_val_iou)+1e-6)\n",
    "    all_val_dice = sum(all_val_dice)/(len(all_val_dice)+1e-6)\n",
    "    print(f\"Val IoU: {all_val_iou}\")   \n",
    "    print(f\"Val dice: {all_val_dice}\")    \n",
    "    val_loss_final = sum(val_loss)/(len(val_loss)+1e-6)\n",
    "    val_loss_formated = \"{:.4f}\".format(val_loss_final)\n",
    "    print(f'Training Loss: {train_loss_formated}')\n",
    "    print(f\"Validation Loss: {val_loss_formated}\")\n",
    "\n",
    "    # TensorBoard writer \n",
    "    writer.add_scalar('Loss/train', train_loss_final, epoch+1)\n",
    "    writer.add_scalar('Loss/val', val_loss_final, epoch+1)\n",
    "    writer.add_scalar('IoU/train', all_train_iou, epoch+1)\n",
    "    writer.add_scalar('IoU/val', all_val_iou, epoch+1)\n",
    "    writer.add_scalar('Dice/train', all_train_dice, epoch+1)\n",
    "    writer.add_scalar('Dice/val', all_val_dice, epoch+1)\n",
    "  \n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping(val_loss_final, model)       \n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "        \n",
    "# load the last checkpoint with the best model\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/aerialmodel.pt'))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "##\n",
    "## __5. Results.__\n",
    "### Model trained for around 250 epocs (2.5 hours) with a batch size of 28 and fine tuned for couple more hours with a batch size of 56. \n",
    "### Result metrics are:\n",
    "#### Train IoU: 0.9059\n",
    "#### Train Dice: 0.9501\n",
    "####\n",
    "#### Val IoU: 0.7506\n",
    "#### Val Dice: 0.8558\n",
    "####\n",
    "#### Training Loss: 0.0534\n",
    "#### Validation Loss: 0.1467\n",
    "####\n",
    "#### Difference between train and val metrics is mostly due to small val dataset, not overfitting.\n",
    "### Now we will look ar by class IoU and make per pixel classification confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1deah4CcRe8n",
    "outputId": "42804ca7-9916-4f11-86a4-2813d239f96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By class IoU:\n",
      "Vegetation   0.694488699441202\n",
      "Land         0.7508653792523501\n",
      "Road         0.5462118738509806\n",
      "Building     0.638781656855028\n",
      "Water        0.8631276465130994\n",
      "Unlabeled    0.0\n"
     ]
    }
   ],
   "source": [
    "iou_metric = IoU(num_classes, normalized=False, ignore_index=None)\n",
    "iou_metric.reset()\n",
    "with torch.no_grad():\n",
    "    for inputs, masks in val_loader:\n",
    "        model.eval()       \n",
    "        masks_one_hot = torch.tensor([])\n",
    "        for every in masks:\n",
    "            masks_one_hot = torch.cat((masks_one_hot, rgb_to_onehot(every.numpy()).view(1,num_classes, image_size, image_size)))\n",
    "        inputs, masks_one_hot = inputs.to(device), masks_one_hot.to(device)\n",
    "        outputs = model(inputs)\n",
    "        iou_metric.add(torch.softmax(outputs,dim=1), masks_one_hot)\n",
    "\n",
    "\n",
    "iou, mean_val_iou = iou_metric.value()\n",
    "print(f\"\"\"By class IoU:\n",
    "Vegetation   {iou[1]}\n",
    "Land         {iou[2]}\n",
    "Road         {iou[3]}\n",
    "Building     {iou[4]}\n",
    "Water        {iou[5]}\n",
    "Unlabeled    {iou[6]}\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "### The smallest IoU is for unlabeled class and as we will see lower it mostly got classified as land and buildings. It probably is some special class of buildings in Dubai or just hard to identify objects, so this result is understandable.\n",
    "### Second and third lowest IoU is Road and Building classes which are also anderstandably some of the hardest to get accurate since they  are pretty small on theese photos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "344dllTHDdg0",
    "outputId": "9283ad00-9f6e-4f2c-df9a-fded421aac71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Bord.       0.99      0.99      0.99   1330085\n",
      "      Veget.       0.88      0.77      0.82   1408415\n",
      "        Land       0.83      0.89      0.86   3921249\n",
      "        Road       0.74      0.68      0.71    908570\n",
      "      Build.       0.82      0.75      0.78   1158302\n",
      "       Water       0.89      0.96      0.93   1429651\n",
      "    Unlabel.       0.00      0.00      0.00     83728\n",
      "\n",
      "    accuracy                           0.86  10240000\n",
      "   macro avg       0.73      0.72      0.73  10240000\n",
      "weighted avg       0.85      0.86      0.85  10240000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVgAAAEGCAYAAAAg6I3HAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3gV1faw331aek9IIwlBkiCXqlgQkCDdhl71InhtqIiCFbCBylUE9WK5CGK56rUL2LFAAOkaBIWAIAkhpBdI78kp+/tjQpKThOQgafy+/T7PPMmZWbPXnpk1a9asvWdvIaVEoVAoFO2PrqsroFAoFP9XUQ5WoVAoOgjlYBUKhaKDUA5WoVAoOgjlYBUKhaKDMHR1Bboj/r562SvM2CW6k/a7dolehaKzKaMoX0oZcCZlTBjtJgsKrQ7J/ra/Zr2UcuKZ6DtdlINtgV5hRn5dH9YluieEDO4SvQpFZ7NRfp52pmUUFFr5dX24Q7L64CP+Z6rvdFEOVqFQnLVIwIatq6txSpSDVSgUZy0SiVk6liLoCpSDVSgUZzUqglUoFIoOQCKxduPP/ZWDVSgUZzU2lINVKBSKdkcCVuVgFQqFomNQEaxCoVB0ABIwqxysQqFQtD8SqVIECoVC0SFIsHZf/6oc7Omwe7MHbzwZitUmmDS1gCn3Hbfbnpdp5OWHwykpMODhbeWR19IICDED8N9Fwfy6yROAaQ/mETu5uEUd/5idx+rlgXbrjCYb85alEzWgitIiA4tnRpCXaQJgyuw8Jk4txGoTrFwQwm9bNR1DY0uZ+Ww2ep3kx09968uc80o6A4dVUFGmjfOz9MFwUg66AHDDPce55ZFchJDk5xi5bVi/M6qH0cnGS18mYzRJ9AbJ9u+9+XBpUF1pksdfT2P45SUA7NniwcLbenfYcT+6PI2oQVVYzYLEfS7s+N6bGQuzcXG14uljJeOoEwA7f/Di41eCOkW3u6cVmw1KCw1UVeh47bGepBxy6ZTr7e5l4eGXMwiOqMVcI3jp4TDSEl1atMmWOJW+zkb7kqv70i1H0xJCWIUQ+4QQCUKI34UQl5xhef8TQlx/JmVYrbDiiZ4s+jiFt7ccZvM3PqQlOdnJvP1MKGOvL+SNTYnc9FAu7y0JBmDXRk+SD7iyckMiy74/whdv9Kg3+KaMnlxMeFS13boJUwspLzZw+/Bz+fJtf+5YkA1AeFQ1sZOLmTE6hvnTIpm9JAudTqLTSWYtzmLBTZHcFRvTrMy3nw3m3nEx3Dsupt65enhbuPXRHB6d0ptrogZQU60743qYawSP3HAO94yL4Z5xMQyNLaPveRVaWTcWcuHYMu4aFcM1UQMIjqjt0OP+6Usf7hwZw92XRWNytjH31XQW3BTJ0gfDMZsFz98bwb3jYvj4laBO0/3c3RGUFelZPDOCj18J5IEXMzvtet94/3GOHnThnrEx/PuBcO55JrtFe2yJtvR1LgKrg0tX0C0dLFAlpRwspRwEPA4scXRHIUSHROWJe10J6VVDcEQtRpMkdnIRv6z3spNJS3Ji0PByAAYNL6/fnp7kxICLy9EbwNnVRuS5VezZ7Nmini3feDNsQondumETStiwxgeA7d95M3hEOSAZNqGELd94Y67VkZfhRHaqiZghlcQMqSQ71URuuhMWs67FMpsy5b488nNMHPrVHYtZx09f+JxxPUBQXakHwGCU6I2Sk+0R1919nJSDLuSkObebvtaOe/dPnoAABMUnjFSW68lNd8JqERTmGrpE94F4d376Ujvuw7+74h9c22nXOzyqmoQd7gBkJDsTGFaLt7+51X1O8lf0dRRaI5dwaOkKuquDbYwnUAQgNP4thPhDCHFACDGlbn2sEGK7EOJb4FCd3HIhRKIQYiPQ40wrUZBrrH/dB/APNpOfYz+kYe9+1ez8UXOqO3/0orJcT2mhnt79qtmz2YPqSkFJgZ6En905kd3ycIj5OUb8g+0N3T/IUi9vswoqSvV4+lrxDzZzItvUaF8TfkFm/IKarrcv87bHclm5MZG7F2ZhNGkvWOFRNdTWCF78PJnl65IIDKs943qAFu28viGRVfsPsnebO4l73QAICLFgcrbx2o9JLPooBYtFdPhxA+gNkovHl5Le6O3DP9jMdXefYNFHKUREV3eq7pNyE6cWsrvuodsZ1/vYIZf69EzM4EoCeza/3qfCkWPtLLR+sN03gu2uOVgXIcQ+wBkIBi6rW/93YDAwCPAHdgshttVtOw/oL6U8JoT4OxAD9AMCgUPAu60pFELMAGYAhIf+tdMy46ksVszvyYZVvgy4uAL/4Fp0ejg/tozEBFceujoaLz8L555fgU7/l1ScMe8tCabwuAGjSfLAi5n8Y9ZxPn4lCJ1O4uVr4b5JUTi5SN786TB7t3ucsT6bTXDvuBjcPK08/c4xImKqSEt0QW+U2KzwwJXRDJ9UzPQncvh925nra4v7lmSSfsSJwhOaA0s+4MIbC0M452/V/LrJk6ffPcb0Eed2iu6T+AWZGTyinIev6dPuOk91vVct78E9z2bx+oZEjv3pQvIfLthsXeOEzhRbF0WnjtBdI9iTKYK+wETgAyGEAEYAn0oprVLKPGArcEHdPr9KKY/V/X9pI7ls4Ke2FEop35JSDpVSDg3wa+79tKd2w43R0lPbL8jCU++k8vqGJG57LAcAdy9tpJ9pD+SxcmMiz686ipSCnr1bzlm1FBnn5xrqo2edXuLmaaW0UE9+jpGAkNpG+9ZSkGusi7ZrWyyz8LgREJhrdcSt8iVmcCUAuRkmKsv11FTpKS00kJ9jQjSx29OtR2MqSrXI/YLRZQAU5xsw12rmt/NHL3qEduxxA9z0cC5efhY+W9ajXq6yXI+Xr5X8HCO7f/JEb5R4+lo6RTdA1KAqBlxcwcLbIykrMrT7cZ/qeleW63npoXDuHRfDv+8Pw8vPQm5aQ1TaGm0da2fS3SPY7upg65FS/oIWrbY18nlFR9YjZnAlWcecyE03Ya4VbPnGh4vHl9rJlBTosdU1aX72Wg/GTykEtAay0kLNaacccubYn86cP6qsRT2xk4uJj7PP7cbHeTHuhiIARl5ZXJc7E8THeRE7uRijyUZgWA2hkbUk7nUlcZ8roZG1BIbVYDDa7Mr07XHyoSC5ZGIJqYnOAKz9nz9+gWaCe9Xg5mmhV0w1W9eeWT28fC24eWoPGJOzjfMuLScjWdO39RsvwqKqCQyrYcjIcmySDj3uidMKGBpbxpJ7Izi8161ezj+4tl4uZnAlOp12rTpDd1BEDVfeXMDKBSFkpTh1yHGf6nq7eVoxGDVjnTStkD/i3aksd+y1qjV9nY1EYEXn0NIVdNcUQT1CiL6AHigAtgN3CyHeB3zRItV5QN8mu21rJNcDGA18cib10Btg1nOZPDGtNzarYPyNhfSKqeb9F4OIHlTJsAml7P/FnXeXhCCEZMBFFcxarLUKW82COddGAeDqYeXR19LRn+LMb1vrTVqSM7fMyyUpwYX4OC/WferLI8vSeW/nn5QV61l8TwQAaUnObFvrzVtbErFaBcufCK1/zVsxP5TFn6Sg00PcZ76kJWk31qPL0/HysyAEHD3ozLJHe2plJbqw+Wtv3t6SCMDebe7Er/c+o3r4BpqZ+590dDrQ6WDbWi92bdTyjJ+9FsTASyp4Z1siUsKPn/h26HHf/3wmeZkmXl17BIBDu11Z/EkKHt5aV6nHVqTh4WPhm3cDANEpun17WLDZ4Pp7T3DX09lUVeiZPvzcTrne4VHVzH01HYkgLdGZV+b0dPhesFnFKfV1Bd05RSBkN/zMTAhhBQ6c/Ak8IaX8vi5N8CIwCe3tYJGUcpUQIhaYK6W8sm5/AbwGjAPSATPwrpTycyHEM8AeKeW3p9I/dJCzVFPGKBQdy0b5+W9SyqFnUkbfgc7y7W8dezhcGnn0jPWdLt0ygpVStviuIrWnwby6pfH6LcCWJnKzT1HGU+1VT4VC0bVoHxp030xnt3SwCoVC4Shd1YDlCMrBKhSKsxYpBVapIliFQqHoEGwqglUoFIr2RyKold3XjXXfmikUCkUbqEYuhUKh6ECs3bgfrHKwCoXirOXkl1zdFeVgFQrFWY1N9SJQKBSK9kcb7EU52LOKpP2uXfbJasyerhmVCCD5Rse/R29vbGlZXaYbQOi77ibVBfh3mW5LRmaX6W4PJAJzyx9+dguUg1UoFGctUqI+NFAoFIqOQagPDRQKhaIjkKgIVqFQKDoM1cilUCgUHYBEdOsBt5WDVSgUZy3atN3d141135opFApFm3TdhIaOoBysQqE4a5F07y+5um/NFAqFwgHaa9puIcREIUSiECJZCPFYC9vDhRCbhRB7hRD7hRCXt1WmimAVCsVZi5SiXSJYIYQeWIE2UWomsFsI8a2U8lAjsQXAainlSiFEP+AHoFdr5SoHq1Aozlq0Rq52+VT2QiBZSpkCIIT4DJgMNHawEvCs+98LyG6rUOVg67DlRk8E/gPoH5ztwi8r7LcbTTbmLUsnakAVpUUGFs+MIC/TBMCU2XlMnFqI1SZYuSCE37Zq12BobCkzn81Gr5P8+Kkvq5cHAjB4RBl3PpmDTiepqtDx0oPhZKc60f+ich70zyHEWMn7Rb3ZV+1rV4eKn23kLbWCDbyu0eF3m71hHX/JSuVvNu14qsFaCFFbjFTusXH8ZWu9XG0qBC/W4xH7157851+Yx933H0Cnk6z/PoI1H0fbbe8/KJ8Z9x0gsncpz/9rKDu3hp6+jlEl3PN0Ojq9ZN1nAaxeGWy33WiyMfflFKIGVFJaZGDJ7HPIy3TCw9vCgjeSiR5YwYbP/Xn9qYj6fWKvLmDKrByQUJBn5MUHe1Na1Hzsh/MvLWbmU2nodJJ1q3uw5o2QZrrnLD1KVP8KSosNLLkviuNZTgwZUcLt89IxmCSWWsE7z4eT8IsXAC98cgjfHmZqqrVzPv/WvpQUtKD74uPMePgQOp0k7tsw1nzQx267wWhlztMJ9OlbQlmJiecXDOF4jit6vY375++nT0wper2NTT/2ZM37DfvqdJJX/7eDghPO/GvOBad5NZpzKtvufE5rTi5/IcSeRr/fklK+Vfd/KJDRaFsmcFGT/RcCcUKI+wA3YGxbCrs0B1uXz5jQZN2DQoiV7VT+YEfyJLbc6JOvB5OAftdf40R4VLWdzISphZQXG7h9+Ll8+bY/dyzQHl7hUdXETi5mxugY5k+LZPaSLHQ6iU4nmbU4iwU3RXJXbAyjJxfXl3nfkkxemBXOveNi2PyVD1MfyAPgRJaJj4t78VuVX7M6Sqsk7wUrPZcZiFxjoGy9jZoUaSfTY46eXp8Y6fWJEZ9/6HAfreWdXIfq6teHrTQgnMHt4r/W8qrTSe59KIGn5g1j5i1jGDUmk7CIUjuZ43kuvLz4PLZs/GuDx+h0klnPprHg1ihmjO1P7NUFhEdV2clMmJJPeYmB6aMG8tU7gUx/TLs3amsEHywN5e3nwuzL1EtmPp3OozfGcM/E/hw77MrVtx5vWfe/Unny9hjunjCQ2KsKCO9TaScz/h8nKC81cMdlg/n63WCmP5oOQGmhgYV3xXDvpIG8NO8c5r501G6/Fx86h9lXDmD2lQNadK46neSeeQd5+sELuefGUVw6PpuwyDL74746g/IyI3ddP5qvP4vk9lmHARgxJgejycasmy7lgVtHMumadHoEN9T76inHyEh1b/W8O0prtt3ZaI1cwqEFyJdSDm20vNVG8U2ZCvxPStkTuBz4UAjRqg/t6kauT4Ebm6y7sW59ezAY7US0xYVAsi4oKUUXlFT7xTc1DJtQYicwbEIJG9b4ALD9O28GjygHJMMmlLDlG2/MtTryMpzITjURM6SSmCGVZKeayE13wmLWseUb7/oyJQJXDy2idPOwUpin3Wx5mSayLa7Yu02N6oMSY5jA1FMgjAKP8TrKt9pOeUClcRLPCc0vb9kmidslAp3zX3Ow0ecWkZ3lTm6OGxaLjm2bejJsRK6dzPFcN1JTvLC1dCAOEDO4gpxUJ3IznLGYdWxd68uwcUV2MsPGFbHxC20Uqu0/+DJ4eBkgqanSc3CPB+Ya+2MXQoIAZ1cbIHF1t1KQ19zJRQ8qJzvNuUH3d75c3FT32Ea6f/Rl8CWlgOToITcKj2tvNWlJLjg52zCaTn2NmunuV0x2piu52a7aud0QwsWX5tnJXHRpHpu+1x5cO34KYtAF+VBnMc7OVnR6GyYnKxaLjsoK7QXVr0cVFww/zvpv7B86f5XWbLsrsKJzaGmDLKDxCepZt64xdwCrAaSUvwDOQKtDoXW1g/0cuEIIYQIQQvQCQgAXIcQvQojfhRBrhBDuddsvF0IcFkL8JoRYJoT4rm69mxDiXSHEr3UtfJPrynwGmCKE2CeEmNJKPexeD7JzbPgHm+0E/IMsnMjWbkibVVBRqsfT14p/sJkT2aZ6ufwcE35BZvyCmq431pf56pyeLPrwGB/tOcSY64tYtbxHmyfKchyMjd7CDD0EluYBGADmHIk5S+J6QXMnWhZna9HxOoqffxX5x13qf+efcMYvoKqVPf6CjqBaTuQ0P6f2Mg3n12YVVJTp8fSxnLJMq0XH8gURrFz/B5/sTiA8qor1qwKayfm3pDuwie7AWvJzGnRXtqB7xKRCkg+6Ya5tONcPvZjC8u8OMHV2FrTwGPXrUU1+XqNze9wZvwD7yNAvoJoTx53rdOuoLDfi6WVmx6Zgqqv1fPT9Jv737U98+XFvyku1Os546BDvLT8X2U5fPLVm253NyS+5HIxgW2M3ECWEiKzzHTcC3zaRSQfGAAghzkVzsCdaK7RLHayUshD4Fe3VHLSDigPmA2OllOcBe4CHhRDOwJvAJCnl+UDju2M+8JOU8kJgNPBvwAg8BaySUg6WUq5qrS57EqpjhBB7hBB7rHSssVw7I58FN0fyz6H9iFvly4yFbebKT4vS9TY8xugQenujsuRLapIlbsO6b8fsjkJvsHHFP48z+/K/Me2CQRw77KrlYzuA8KhKpj+SwWvzI+vXvfhQH+6dNJB5U/rR/4JSxlyb3646o/9WjM0quPmKMUy/djTXTkshKKSSC4bnUVJoIvmwV7vq607Y0Dm0tIaU0gLMBtYDf6L1FjgohHhGCHF1ndgc4C4hRALaW/ZtUspW39O6OoIF+zTBjWiRZD9gpxBiH3ArEAH0BVKklMca7XeS8cBjdfJb0J4s4adRh6yhg5wrT+ZmwoKdyM+xf33MzzUQEKI5Xp1e4uZppbRQT36OkYCQ2no5/+BaCnKNFOQ2XW8mP8eIl6+F3v2qSNzrBsDWb73pN7SizQoaeoC50dui5bjEcIrAV4tSW4heN9hwHy0Qhr/uYAvyXfDv0RCx+gdUU3DCpZU9/oKOXBMBwc3Pqb1Mw/nV6SVuHlZKi07dZntOPy0fmZPuDAi2fefLueeXN5PLb0l3k1RCQZ4J/+AG3a6NdPsH1fDkG0dYOvecOl0N+wBUVejZ/K0/0YOaX/OC4874BzY6tz2qKTjhbC9zwpmAHtV1um24upspLTESOyGb3+IDsFp1lBQ5cWi/D33OLabfoCIuuvQ47371E48u2svAofnMXbj3lOfJEU5l212BlGC26Rxa2i5L/iCljJZSniOlfK5u3VNSym/r/j8kpRwupRxUF7TFtVVmd3Cw3wBjhBDnAa7A78CGugMYLKXsJ6W8o40yBHBdo33CpZR/nkYddgNRttzoSFtutOm6yU7Ex9k/8ePjvBh3g5aLG3llMQk73AFBfJwXsZOLMZpsBIbVEBpZS+JeVxL3uRIaWUtgWA0Go43YycXEx3lRVqLHzdNKaO8aAM67tIyMI860hXM/gTlDUpslkWZJWZwN90ubX76aVIm1DJwHNneipetbzsueDkmHvQnpWU5gcAUGg41Lx2QSvzPojMpsSmKCGyGRNfXnbtRVhcRv8LGTid/ozdjrtChw5OWFJPzsAa10Js/PNRERVY2Xr/aQPG9kCRnJzc970n53QnpVE9izWtN9ZSHxG5vo3tRI96RCEn7xBARuHhb+9U4S770YxqHfPOrldXqJp4+mV2+wcdFlRaQlNX8oJf3pRWhYBYHBldq5HZfNrm32rfO7tgcy5gptFoIRl+Wyf48/IDiR68KgoQUAODlb6Nu/mMw0d95/vS+3XjWG6ddexgsLhrB/jz9LFw455XlyhFPZdlegpQh0Di1dQZd305JSlgshNgPvokWl8cAKIUQfKWWyEMINLUeaCPQWQvSSUqYCjXOq64H7hBD3SSmlEGKIlHIvUAZ40Aa6oCSLLTf65OuB/qu1NaQlOXPLvFySElyIj/Ni3ae+PLIsnfd2/klZsZ7F92jdf9KSnNm21pu3tiRitQqWPxGKzabd6Cvmh7L4kxR0eoj7zJe0JO2GfnVuGE++nYq0QVmJnpcf1nLr0YMqeSYwFRdhpb9zMZOs2Sw50R8AYRD0mKcn8z4LWMHrah1O5wjy37DifK7AfZRmQGXrbXiO1yGEvbMxZ0sseRKX884sPWCz6lj56kAWLf1Z60r0QwTpqZ78c/qfHEn0ZtfOYKL6FvHkol24e5i56JJc/jn9MPfcOuY0dAhefyqc5z5I1M7dan/Sjrhw88NZHNnvSvxGH9atCuCRV1J4d+t+yooNLJndu37/93ck4OphxWCUDBtfxPybY0g/4sJHr4bw7zWHsZoFeVkmXprTu0XdKxf2YtH7ieh1krg1AaQfceXmBzNJOuDGrk0+rF/Vg3kvH+Wdn/ZRVmLg+fu17lBX3ZJHSEQ10+7LYtp9WvvI/Fv7Ul2pY9H/DmMwSnQ62LvTk3WfNX/9sFl1rFzan2eX/YpOJ9mwtifpxzz454xEjvzpza7tgcR9G8bchft4+/PNlJUaeXHBeQB893kEDz2ZwOufbkUI2PBdT1KTPZvpaA9sVnFK2+4KuvNYBKKNFELnVEKIa4CvgHOllIeFEJcBLwBOdSILpJTfCiGuQsuvVqBFnR5SypuEEC7Aq8AlaFH5MSnllUIIXzSnaQSWAEeBmVLKO1urj6fwlRcJxx1Ce6Lm5Ooa1Jxcnc9G+flvUsqhZ1JGQD8/ed2HjnQUgjeHfnTG+k6XLo9gAaSUX9Po/U5K+RPQUm/ozVLKvkILz1agNYAhpawC7m6h3MIWymnVuSoUirOJ9vlUtqPovjVrmbvqGrIOon2q9mYX10ehUHQxtrp5udpauoJuEcE6ipTyFeCVrq6HQqHoHmi9CNS03QqFQtHuqCljFAqFogNR03YrFApFB3BysJfuinKwCoXirKY79yJQDlahUJy1SCmwKAerUCgUHYNKESgUCkUHoHKwitMiaXjXve7MO9R0+MvO46XzR3SZbgBpPvVYsh2NJatjhk38/wXlYBUKhaIDUP1gFQqFogNR/WAVCoWiA5ASLA4Mpt1VKAerUCjOalSKQKFQKDoAlYNVKBSKDqS9ZsvtCJSDVSgUZzWqkUuhUCg6AClVDlahUCg6CIFV9SJQKBSKjkHlYBUKhaIDUGMRnCXYcqMnAv8B9A/OduGXFfbbjSYb85alEzWgitIiA4tnRpCXaQJgyuw8Jk4txGoTrFwQwm9btfnoh8aWMvPZbPQ6yY+f+rJ6eSAAL32VjIu7FQBvPwuJ+1z51/RIhk0o4dZH87DZBFYLbFnrx+VTT6DTSdatCmD1GyHN6jT3pRSi+ldQWmxgyew+5GU5MWRECdMfycBglFjMgv8uCSfhF61Ot87NYOy1Bbh7Wbi2v+MzGKdsdWfTsyFIKwycUsTFM0/YbS/NNvL93J7UlOmRVrh0Xh7njC7DWitYvyCE3AOuCJ1kzJM5hF9c0aa+80cUcvfjR9HpJes/D2LNf8PtthuMNuY+n0ifv5VRVmxkycPncjzbuX57QHA1b6zdw8crIvjyvTCMJhsvfpCA0WRDb5DsiPPn4+W9WtY9soiZC46h08O61T1Y85b9dOZGk405Lx5pOO8PRHM8y5nogWXcv+gooE2R/PFrYfy8wQ+jyca/P/mjQfc6Pz5aFt6CZhgaW8LMf2Wi18OPn/qxekVQM93zXk0lamAVpUV6Ft8TSV6mNrv9lFm5TJxagNUKK58Kq7fDa+44zqSp+QgBP37iz1fv9Gjz/LfFqWy705FaHra70q0drBCiXErp3s5lLgTKpZRLT66z5Ubr0aYBHwdkXn+NU01WXDXpRxpu2AlTCykvNnD78HMZNbmIOxZks3hmL8KjqomdXMyM0TH4Bpp5flUKd4zwAGDW4iwev7E3+TlGXvvhCPHrvUg/4syca/vUl/vk26n8sl67EfZudyd+SwAg6N2vgmVfH+LOMQPIzzWx7JuDxG/0IT3ZpaFO/zhBeYme6aMHMerKAqY/lsGS+/pQWmjg6TujKTxuIiK6kufeT+Sfw4YAsGujD2vfD+SdzfsdPmc2K2xcGMI/3j+GR5CFD649hz5jSvGPqqmX+Xl5D/peUcKQmwrJP+LE53f04pzRiSSs8gFg+o9HqMjX8/n0SG75OhnRStpMp5PcuyCZ+XcOID/PiVdX7SV+sx8ZR90ajv26XMpLDdw58UIunXSc6XOO8fycc+u33/VICnu2+9b/NtcKHp8+kOpKPXqDjaUfJbBnmy+J+z2b6Z61MIUnbvsb+bkm/vPFfnb95Et6smu9zPjr8ygvNXDH2PMYdUU+0+el8fyDMaQluXL/tYOwWQU+AbW8vnYf8T/5Yq4VPHbL3xp0f/YHe7b5cHifR3PdizJ4fFqUZjPfJxIf50X6kUbX/MYCyksM3D7ib4y6upA7nshi8b29CY+qInZyETMuO1ezw0+PcMelfyMsqppJU/O5/8q+mM2CxR8ls2uTJ9mpzvxVdDp5StvuCrpzL4Lumx3uXC4EknVBSSm6oKTaL76pYdiEEjuBYRNK2LBGcxbbv/Nm8IhyQDJsQglbvvHGXKsjL8OJ7FQTMUMqiRlSSXaqidx0JyxmHVu+8W5Wpqu7lUHDy/l5nRcA1ZV6qDOW6AHlmM2C3AxnLGYdW9f6MWxckX2dxhWx8Qt/rU4/+jL4klJAcvSQG4XHteg6LckFJ2cbRpMNgMP73Ck8YTqtk5OT4Ip3RC3e4Wb0Jsm5V5aQvNHeMQ1AvPIAACAASURBVAkhqS3XzKmmTI97DzMABcnORAzTIlY3fytOnlZyD7jQGtEDyshOdyE30wWLWce2HwMYdlmBnczFlxWw8WstatoRF8Cgi4vQXhhh2Jh8crOc7ZwiiLrzCwaDRG9oOeyJHlhOdppLw3n/3p+LxxTayQwbW8TGL7UocPs6PwYPKwEkNdV6bFbt+pmcbI1yg/a6DQbZYtQVM7iC7FSnRjbjw7DxTexwfDEb1mgPju3f+zB4RBkgGTa+hC3f+DSyQydiBlcQ3qeaw/vcqKnWYbMK9se7M3xS8alOvUM4Ytudhaxr5HJk6QrOOgcrhLhKCLFLCLFXCLFRCBFYt36hEOJdIcQWIUSKEOL+RvvMF0IkCSF2ADEtFBsKZJz8kZ1jwz/YbCfgH2ThRLYRAJtVUFGqx9PXin+wmRPZDQ4rP8eEX5AZv6Cm643NyrxkYgn7drhTWd4w7fAl4wt5e+N+7n4yg4SfGyKc/FwTfkG1dvv7BZo5kePUUKcyPZ4+9sPujZhURPIfbphr//qlLs8z4NGo7h5BZsryjHYywx84zsGvfXh9eF8+v6MXY5/OBiCgbxXJmzyxWaA4w0jeHy6U5tjv2xS/wBryc53qf+fnOuHXo+mx13Ait+HYK8sMeHpbcHa1cv0dGXzyekSzcnU6yWtf/sYnO35h78/ezaJXAP+gGk7kNLpuuSb8Apvrzs81NegubzjvMYPKeOOHvaz8bh/Ln+pd73B1Osnyb/fxafxu9u70IjHBPnoF8As2N9Hd3Gb8gxpk6u3Qp84OG53X/FwTfsFmUhOd6X9hOR7eFpycbVxwWSkBIfZlni6O2HZnIqVjS1dw1jlYYAdwsZRyCPAZ8EijbX2BCWgR6dNCCKMQ4nzgRmAwcDlwQUuF7kmojhFC7BFC7LHSOcYSe00xW772tlv3c5wvd40dyJo3g+jTv/KMyo+IqmT6oxksm9/rjMpxhD/XetP/uiLu3XmY699J5fu5YUgbDLyhCPcgMx9c04efFoUQel4lug60uptmpfH1Bz3rI8bG2GyC+/5+PreMvpjoAWVE9Gk7F3y6JCZ4MPPyITxw3UD+cXdW/ZuDzSaYffVgbh45lOiB5UREtb/ulshIdmH164Es+eQIz32UTMpBF2zWTlHdaUgpHFq6grPRwfYE1gshDgDzgL812va9lLJGSpkPHAcCgZHAV1LKSillKdDSqNJZQwc5V0oph0oph4YFO5HfJMrKzzXUP/l1eombp5XSQj35OUYCQhoiHP/gWgpyjRTkNl1vtivT09dCzOBKdm1qHkUB7N3hhYeXBU8fTad/UC0Fufav9gV5RgKCaxrq5GGltMhQL//km0dYOqc3OelnlhtzD7RQ1qjuZblGPALtH0L71/jQ93LtNTH0vEosNToqC/XoDDBmQQ63fZfM399Mo7pUh09kDa1RkOeEf1CDjH9QDQXHmx67EwFBDcfu6mGhtNhAzMBSps9J4b0Nu5h8cxZTZmRw5bQsu30rygzs/9Wb80fav/qDFi0HBDe6bkG1FOQ11+1f9zah00tc3RvO+0kyjrpSVamjV7T9Q7KizMD+XV4MvbT5a3pBjrGJbnMLdtggU2+HRXV22CiK9A+qpaBu3/Wf+TP78nOZe3005SV6MlPOzB7asu3ORItOlYNtT14DlkspBwB3A42tpfGda8XxRrzdQJQtNzrSlhttum6yE/FxXnYC8XFejLtBy4GOvLKYhB3ugCA+zovYycUYTTYCw2oIjawlca8riftcCY2sJTCsBoPRRuzkYrsyR15RzK6NnphrGi5BSK8aTuYRrRaBwShxcbNiMNoYdVUB8Rvto934jT6MvS5fK29SYV1PAYGbh4Vn3k3kvRfCOPRb81fR0yV4YCVFqU4UZxix1gr+/M6LPmNK7WQ8g82k/aw1QhUkO2GpEbj6WTFXCWorNeNO3eGOzoBd41hLJP3hQUhEFYGhVRiMNi6ddIL4zX52Mrs2+zH2mjwARow/wf5d3oDgkZsHc/u4i7h93EV882Eoq94K47tPQvH0qcXNQ3uNNzlZGXJJEZkprk1Vk3TAnZBeVQT2rNbO+xX5xG/ytZOJ3+TD2L8fB2DkxAIS4r0AQWDPanR67fr1CKkmrHcVeVlOePmam+guJiOleR46McGN0MiaRjZTRPyGJna4wZtxN2gPhpFXFJGw0wMQxG/wInZyUSM7rCFxn3Y9vPw0xxsQUsvwScVs/tqn1fPfFm3Zdmdjk8KhpSvo1r0IToEXcDIkudUB+W3A/4QQS9CO9yrgzcYCuqAkiy03ejawHtB/tbaGtCRnbpmXS1KCC/FxXqz71JdHlqXz3s4/KSvWs/geLceXluTMtrXevLUlEatVsPyJUGw27WKumB/K4k9S0Okh7jNf0pIangWjJhezerl9d5kRV5Qw9oY0LBZBbbXg3RfDeO6DRHQ6iFsTQNoRV25+KJMjB9yI3+jDulUBPPLKUd7dnEBZiYEl950DwNW35hESUcO0+7OZdr+WC33ilhhKCozc8Vg6sVcX4ORi48Of97J+VQAf/ce+G1JTdAYY+3Q2a26LRNpgwPVF+EfXsP2VHgQNqCJqbBmjn8hh/ROh7HnPHyHg8hczEQIqCwysvi0SoZN4BFq44qWMVnWBlltc+VwfFr39BzqdJO6rINKT3fjn7FSOHPRg12Y/1n8RxNwXDvPfdb9SVmzkhbl9Wy3TN6CWOUu0cyl0ku3rAvh1q18zOZtVsPJfvVn07iH0eknc54GkJ7ty8wPpJB1wZ9dPvqxfE8i8pUd4Z+PvlBUbeP6haAD+dn4p/7g7C4tFIG2CFQt7U1pkpFdMBXNfTEank5ruH/35dbNvi7pXPBnG4o812bhVfqQluXDL3GySElyJ3+DNus/8eOQ/qby346Bmh/dGAlpj5ra13rz10yHNDheE1dvhU2+l4OFjxWoRLJ8fRkXpmd32Nqto1bY7m+7cTUvIblw7IYQNyG606mXgKPAKUAT8BFwgpYxt2v1KCPEHcKWUMlUIMR/NGR8H0oHfG3fTaoqn8JUXiTEdcUhtIpyc2hbqIOYd2tNluv9/npPLVlXdZbq7MiG7UX7+m5TS8c7YLeDcJ1T2evFuh2QTr3v6jPWdLt06gpXylBOef9OC7MImv/s3+v854Ll2rZxCoegWtFeIKISo/9gI+K+U8vkWZP4BLKxTmyClnNZamd3awSoUCkWryPYZi0AIYfexEbBbCPGtlPJQI5ko4HFguJSySAjR5idxZ2Mjl0KhUDQgHVxa50IgWUqZIqWsResCOrmJzF3ACillEYCU8nhbhSoHq1AozmraqZuW3cdGaFFsaBOZaCBaCLFTCBFfl1JolVOmCIQQr9GK35dS3n+qbQqFQtEZSKjvLeEA/kKIxi25b0kp3zoNdQYgCohF64+/TQgxQEp5ym+PW8vBdl2TskKhUDiCBBzPwea30osgCwhr9LsnDd1BT5IJ7JJSmoFjQogkNIe7+1QKT+lgpZTvN/4thHCVUp7Zt5sKhULRzrRTT9PdQJQQIhLNsd4INO0h8DUwFXhPCOGPljJIaa3QNnOwQohhQohDwOG634OEEK+ffv0VCoWiA2iHRi4ppQU4+bHRn8BqKeVBIcQzQoir68TWAwV1/nAzME9KWdByiRqOdNN6FW0AlW/rKpIghLjUgf0UCoWig2m/cQaklD8APzRZ91Sj/yXwcN3iEA71g5VSZghhdxD/x8bjUSgUZy3d92NUhxxshhDiEkAKIYzAA2ghtKIDEPrmw+x1Fk8+cVeX6Tasyusy3QCuV7Q9RkKHIW1dp/tsR4J0vBdBp+NIP9iZwCy0PmHZaOOqzurISikUCoXjCAeXzqfNCLZubNWbOqEuCoVCcfp04xSBI70Iegsh1gohTgghjgshvhFC9O6MyikUCkWbtM+nsh2CIymCT4DVQDAQAqwBPu3ISikUCoVDnPzQwJGlC3DEwbpKKT+UUlrqlo+wn0VAoVAouozuPOlha2MRnBxy/UchxGNoo8tIYApN+oopFApFl9GNexG01sj1G5pDPVn7xsOGS7RxERUKhaJLEd24kau1sQgiO7MiCoVCcdp0YQOWIzj0JZcQoj/Qj0a5VynlBx1VKYVCoXCMrmvAcoQ2HawQ4mm08Q/7oeVeJwE7AOVgFQpF19ONI1hHehFcD4wBcqWUtwOD0KbOVigUiq7H5uDSBTiSIqiSUtqEEBYhhCfa1Ndhbe10NjNmtJFXnz2MXif58VNfVi8PtNtuNNmYtyydqAFVlBYZWDwzgrxMEwBTZucxcWohVptg5YIQftvqCcDDL6dz0dgyivMN3H1ZTH1ZI68s5uY5uYRF1XD/5VEcTXat33b+pUXMXJCKTi9ZtzqQNW+GNqvHnH8nE9W/nNIiI0seiOJ4ljNDhhdz+7x0DEYbFrOOd56PICHeCxc3K//+9I/6/f2Datn8jT9vPtd2uv2ivuk8eO3P6IVk7a6+fLhpiN32G0ft56qL/8Rq01Fc7sziz2LJLfLgvD5Z3H/NL/VyET2KefqDMWz7w/EUv253JcY3CsEqsU7ywDLFu5mMfms5ho+0geVtvU2YH9fmozP8txD9rkqQYDvPBfM9viBaf6U8f1QJ9yzMQKeHdZ/5s/r1ILvtRpONua+kEjWgktIiPUtm9SYv0wkPbwsL3jhK9KBKNqzx4/Wnwuv3eXFVIr49zNRUazHNE/+MoqTACMDQ2FJmPpNVZ29+rF7Rgr39J71On4HF90SQl6lN7z5ldh4TbyzQ7O3J0Hp7ez/+IFXlemw2sFoE912u2dzIK4u5+eFcwqKquf+KaI4kuDh2EZowNLaUmc9mn/Ie6TROb8DtTscRB7tHCOENvI3Ws6Ac+KX1XdoHIYQVOIBWz2PAza1Nz3Aa5ZZLKd1b2mbLjdYvXezOghsjyc8x8toPR4hf70X6kYauvxOmFlJebOD24ecyanIRdyzIZvHMXoRHVRM7uZgZo2PwDTTz/KoU7hjhgc0miFvly7fv+TPvP/aDiqQeduaZO3tx/wuZdut1Osmshcd44tZ+5Oea+M+XB9i1yYf0Rg54/A3HKS8xcMeY8xh1RT7TH0nn+QeiKS0ysnBGXwqPm4iIqmTRe4e4ecRQqir0zL56UP3+y77ez844vzbPl07YmHvdTh544wqOF7vxzkNfsv2PXqTm+dTLJGX5Mf3lv1NjNnLtJQe596p4nvpgHL8nh3Lb0usB8HCtZs0Tn7ErsWebOuuxSowrCqhdEoT0N+B0XzbWi12REaZ6EZFlxrCqhJqXg8FDD8XaYG+6g9XoDlZT84b2YHKak4NufzW2Qad2KjqdZNaidJ64KZr8HCPL1h4mfoMX6Uca9pkwJZ/yEj3TL+3PqKsKmf54Fktm9aa2RvDBS6FExFTRK7qqWdkvPBDJkf1uzfU9l8njU8+ps7ck4uNasLcSPbeP6Meoq4u4Y34Oi+85aW9FzLisr2Zvnx3ljpHn1k+h8sgNfSgtsr/FUw8788xdvbj/+b8+uI1OJ5m1OIvHb+x9ynukM+nOvQjaTBFIKe+VUhZLKd9Am9L21rpUQWdQJaUcLKXsDxTSOYPMXJiSaiU33QmLWceWb7wZNqHETmDYhBI2rNGcy/bvvBk8ohyQDJtQwpZvvDHX6sjLcCI71UTMEG0SiD92uVNW1Px5lpHsTObR5oYZPaic7DRncjOcsZh1bP3en4vHFtnXY2whG78K0Oqxzo/Bw0oAydFDbhQe1xxQ2hEXnJxtGE3270ihvarw9jPzx26PNk9Iv/DjZOZ7kl3gicWqZ+PePozsn2on83tyKDVmLSI7mBZID++KZuVcNiiFXw6H1cs5gi6xBhliRAYbwSiwxrqh/8V+Yg39j2VYrvLUnCuAd91fAaJWgkWCWfsrfVofrSxmcAU5qc7113/rWh+Gjbd/pg8bX8LGz7UH0/YffBg8vBSQ1FTpObjbHXO14xFVzOAKslOdGtmbT3N7G1/ChjVat/Tt33szeEQZDfbm08jenOrt7VScyt5Oh5ghlWSnmlq9RzqVs/FTWSHEeU0XwBcw1P3f2fxC3SyPQojBdbM67hdCfCWE8Klbf5cQYrcQIkEI8YUQwrVufaQQ4hchxAEhxKI29IRmZTU4o/wcI/7BZjsB/yALJ7I1J2GzCipK9Xj6WvEPNnMi29RoXxN+Qfb7Oop/YC0ncpwayso14RdYYyfjF1hLfo6pvh6V5Xo8fSx2MiMmFpJ80B1zrf2lHnVlPtu+98ORUYYCvCvJK24I+E+UuBHg1dyBnuTKiw4T/2d4s/Vjhxxlw+992tRnR4EVGdDgFKW/HpFvf4y6TDMiy4zpoWycHshGt1tzMrZ+zlgHOeM8NQPnqelYz3dBhptoDb8gc/21hbprGGhuIlNbf51tVkFFmR5Pn7aHSH54aSorfjzEtPtzOHnHN9dnxD+oqb2Zm9ubj9Vu/cl96+1NChZ/epTlPyYy6ab8Nut2Omh1bmznze8RhUZrKYKXWtkmgcvauS6nRAihR2toe6du1QfAfVLKrUKIZ4CngQeBL6WUb9ftswi4A3gN+A+wUkr5gRCixShYCDEDmHHbFE+fkRe0mD046wiPqmT6I2nMv61fs22jrizg33NO09k5wITzk+gbdoJZy6+2W+/nWUHv4EJ2HT6N9ICjWCW6LDO1/w5G5Fswzcmh5s1QRIkNXYaZ6o+1JgOnx3OxHajGNqDzX2VfuD+SgjwTLm5WFrx5lDHXmdj0Rdvpmb/Kw9f2oSDXhJefljrISHbmj13/N+y6KWdlikBKObqVpbOcq4sQYh+QCwQCG4QQXoC3lHJrncz7wMkpbPoLIbYLIQ6gDbH4t7r1w2kYoObDlhRJKd+SUg5959XAf4aFNkQF/sFm8nPsX2nzcw0EhGhPbJ1e4uZppbRQT36OkYCQ2kb71lKQ6/jrsJ2OPBMBwQ0Rq39QLQV5TnYyBXkm/INr6+vh6m6tz7n5B9Xw5OuJLJ3bh5x0e4cS2bcCnV6SfNCxG+5EsSuB3uX1vwO8KjhR4tZMbmh0JreO28uj70zEbLV/FR8zOIVtB3phtZ3mgOJ+esSJhuhQ5FuR/vZxgfQ3YL3YFQwCGWRE9jQisizof67A1tcJXHTgosM61AXdn9WtqivINdZfW6i7hnnGJjKm+uus00vcPKyUFrV+XAV5WsRXVaFny9e+xAyqOIU+M/m5Te3N2NzeivR260/ue9LeCnI1fSUFRnb+6EXfwe03X6lW58Z23vwe6TQk2qeyjixdgCPdtLqSKinlYCAC7V22rRzs/4DZUsoBwL+wH5TG0efc7nMi9QSG1WAw2oidXEx8nH2vtPg4L8bdoOVDR15ZTMIOd0AQH+dF7ORijCYbgWE1hEbWkrjXtQUVbZO0352QiGoCe1ZjMNoYdUU+8Zt87GTiN/ky9toTWj0mFpAQ7wUI3Dws/Ovtw7z373AO/e7ZrOzYq/LZ+p2/w3X5M6MHPQNKCPYtxaC3MnZIMjsORtjJRIfm8+gN23nkvxMpKm/eiDR2SPLppwcAW4wTIsuMyDWDWaLfUqE500ZYL3FFt7/OcZZYEZlmZLABGWDQ1lu1/KvuQDW2NlIEiQluhERW11//UVcVEb/BvtdC/AYvxl6vzXU38vIiEn72pLVUi04v61M3eoPkwrElpCa51OsLjaxpZG9FxMfZX7P4OE/G3VCo6buimISdHmj25kns5KJG9lZD4l5XnFysuLhpDyUnFyvnjyojNbH9ovbEfa6ERta2eo90Kt04B+vQl1xdjZSyUghxP9q0ua8DRUKIkVLK7cDNwMlo1gPIqZva5iYa5jXfiTYN70e0MXi4LijJMu+mQF75JAWdHuI+8yUtyZlb5uWSlOBCfJwX6z715ZFl6by380/KivUsvkdzNmlJzmxb681bWxKxWgXLnwitb9F97PU0Bg4rx8vXwkd7DvHhS4Gs/9SPSyaWcO+iLLz8LDz74TFSDh9nwe39sFkFK/8VyaL3/kSvl8St6UH6EVdufiCdpD/c2bXJl/WrezDvpSO8s+l3yooNPP9gNABX3ZxLSEQ102ZnMm221jth/m39KCnUooyRkwp46s5zHT7/VpuOl78YwSt3/4BeJ/luVwzHcn25c+JuDmcEsONgL2ZdHY+Lk5lFt20AIK/InUffmQhAkE8Zgd7l7D0a4rDOevQC8yw/TE/kgg2s4z2QvUwY3i/CFm3CNswN21AX9L9X4XRXJujAcpcveOqxjnRDl1CN091ZIMA21AXbxa0/8GxWwetPhvPch0fQ6SVxq/xJS3Lh5oezOXLAlfgN3qxb5c8jrx7j3W1/UFasZ8nshuGR3995AFcPKwajZNiEYub/M4q8TBPPfXQEg0Gi00v27vBk3Sf+9fpWLOjJ4k9S0Okkcat8SUty4Za5OSQluBK/wYt1n/nxyLI03ttxiLJiA4vvPWlvLpq9bT6s2dv8nthsAp8AC0+/c0w7fXrY/LU3e7ZoTvuSicWavflaePaDFI4edGb+tHNO65LYrIIV80O1Oje6R7qK7pwiELKrxvFygKbdqYQQa9HGpj0AvAG4os1LfruUskgIcQ/wCHAC2AV4SClvq5vr/BPAHfgGePBU3bQAPIWvvEiM6ajDahWd61+LeNuDkqsGdpluw/T/f+fkktYunEO0C+//jfLz36SUQ8+kDKewMNnzwYcckk2ZO+eM9Z0ujnwqK9Civt5SymeEEOFAkJTy146uXFMnKKW8qtHPi1uQXwmsbGH9MWBYo1UL2quOCoWii+m+MaJDOdjX0ZzT1LrfZcCKDquRQqFQOIiQji9dgSM52IuklOcJIfYC1L2Kt95SoFAoFJ3FWTrg9knMdf1QJYAQIoAuGzpBoVAo7OnOjVyOpAiWAV8BPYQQz6ENVbi4Q2ulUCgUjnI2d9OSUn4shPgN7UsqAVwjpfyzw2umUCgUbdGF+VVHcKQXQThQCaxtvE5Kmd6RFVMoFAqHOJsdLPA9DZMfOgORQCINn6EqFApFlyG6cYuQIymCAY1/142kdW+H1UihUCj+j3Dan8pKKX8XQlzUEZVRKBSK0+ZsThEIIR5u9FMHnAdkd1iNFAqFwlHO9kYutAFUTmJBy8l+0THVUcja2raFOginYkvbQh2l+8qstoU6kOAdf21uqvYg++KyLtP9f4Kz1cHWfWDgIaWc20n1USgUitOjGzvY1qaMMUgprWiDVSsUCkW3Q6D1InBkabMsISYKIRKFEMlCiMdakbtOCCGFEG2OzNVaBPsrWr51nxDiW2ANUD8Rk5Tyy7arrFAoFB1IO+Vg697WV6BN7JoJ7BZCfCulPNREzgN4AG041DZxJAfrDBSgzcF1sj+sBJSDVSgUXU/7pAguBJKllCkAQojPgMnAoSZyzwIvAPMcKbQ1B9ujrgfBHzQ41pN046yHQqH4/wrHvZG/EGJPo99vSSnfqvs/FGg86nomYNcdte4bgDAp5fdCiDN2sHq0GQBaGgtMOViFQtEtOI0UQf5fndFACKEDXgZuO539WnOwOVLKZ/5KZRQKhaLTaJ9wLwsIa/S7Jw1z+oHWXbU/sEWb5IUg4FshxNVSysZRsR2tOdjuO4qtQqFQgNbI1T5jEewGourm78tCmyR1Wr0aKUuA+qmYhRBbgLmtOVdofTzYrpn1T6FQKE6HdhgPVkppAWYD64E/gdVSyoNCiGeEEFf/1aqdMoKVUhb+1UIVCoWis2ivT2WllD8APzRZ99QpZGMdKfO0B3v5/4Exo428+uxh9DrJj5/6snp5oN12o8nGvGXpRA2oorTIwOKZEeRlatOUTZmdx8SphVhtgpULQvhtqzYf/cMvp3PR2DKK8w3cfVlMM53X3X2cGU/n8I9Bgygt0i7L+aNKuGdhBjo9rPvMn9WvBzWrx9xXUokaUElpkZ4ls3qTl+mEh7eFBW8cJXpQJRvW+PH6U+HN9C18J5mg8BpmjnNs1MkL+mcye1o8ep2N77fF8OkPg+y23zD+AJdfmoTVJigpc+bFd0eSV+DBOWEFPHTLTtxczFhtgo+/G8zmX3u3qe/8USXc83Q6Or1k3WcBrF4Z3PzYX06pO3YDS2af0+jYk4keWMGGz/15/amI+n0uvbKAqbNz0OkluzZ58+7zYU3VNqP6Fwslr1SDDVyvNuJxi5Pd9pJXq6n5TZt2W1ZLbEWS4I3a1+XZl5RhOEd7SdQHCvyWtjwl+9DYUmY+m91u9mZ0svHSl8kYTRK9QbL9e28+XKrZzpxX0hk4rIKKMq1eSx8MJ+Xg6X8m3FadO5Vu3OTuyJQxnYYQwiqE2CeESBBC/C6EuMSBff4rhOhX93+qEMK/BZmFQgiHPve15Ubrly52Z8FNkdwVG8PoycWER1XbyUyYWkh5sYHbh5/Ll2/7c8cCbeyb8KhqYicXM2N0DPOnRTJ7SRY6nXb141b5Mv+myBZ1BoTUct6oMvIyjfXrdDrJrEXpLLg1ihlj+hF7dSHhUVX29ZiST3mJnumX9uer/wYy/XEtJ19bI/jgpVDefq5ni/qGTyyiqsLxS68TNh64+Wcee2U8t82/jjEXpRARUmQncyTdj5nPTObOp/7O1j2R3P2P3QDU1BpY8t9R3L7gOh59eQKzpsbj5lLTuj6dZNazadqxj+1P7NUFpzh2A9NHDeSrdwKZ/lhGw7EvDeXt5+ydp4e3hTufyOSxaTHcPW4APgFmBg8vbbUe0iopWVqN3yuu9PjUjao4C+ZjVjsZrwed6fGhGz0+dMPtBhP/r70zD4+qOhv4750lK9kTkhAIBIUggqBGaUQhLohaFf3EUleodS24lCJVQaWioLZWSxEVd60LrnWpQqAIKBAFyo4GEMKShez7PnO+P+5NMpNtJpCQ0J7f88wzd+5Z3rPdd9577jnv9UtpslnEl8awtpSroJg6L7NTx1tdjTDz2pO4a1wid41LJCmljCFnNO4R4uW5sfxu6eN3AwAAIABJREFUXCK/G5d4VMrVYvFc5uOGt9MD3aSEe5SCBaqUUiOVUiOAB4H5nhIopW5tvtviGDl7X4aDnIO+1NdZWPVZKMnjS9wiJI8vYfmHYQB8+2UoI88tBxTJ40tY9VkodbUWjhzyJSvDh8TTKwHY8X0vyopav2G4Y04Wrz7eB+UyCBJHVpCd4ddYjtVfhJF8cbF7OS4uYcVHEUY5vgozFYaipsrKzg29qKtu+ZzSL8DB/912hPf+HtsirC2GDMwjKzeY7Lxg6h1WVv4wkNGnu7/QYstPfaipNeq36+coosKMC/rwkRAyj4QAUFAcSHGpP6HB7V+MRt19yTnkZ9Y9nORx7go9eVwRKz6ONOsezsjRZU113xhEXY370I6NryYzw5eSQuNPbMt3wYy+tP1ZsLpdTmx9LdjiLIhd8B9no3pN2w5xqpbX4T/O3mZ4awz0KSMrw6eTx5tQXWkFwGZXWO3KbWwdK4mnV3os8/FC6Nmv7e5pCtaVYKAIQERSROTLhgARWSgiU8zjVa3tCRaRWSKyW0S+A1rek7dNXGZm02PJ/Gw7kbF1bhEiY+rJyzIuJKdDqCi1EhzuIDK2jrwsH5e0PkTEuKdtTvL4EvJz7Ozb5W5JRMTUNcpozCu6rlmc2kZ5TodQUWYlOMzdwmrOzTOy+HhxNDVV3nd9ZFgluYWBjb/zCgOIDKtoM/5lY3bz/faW1vOQhDxsNgdZucHtyouIqSUvu/12NNqned3bVn5ZGX70HVhNdN8aLFZF8vhiomLb91zmyHNi7d3UTtbeFhx5rV+p9dlOHFkK3yRr4zlVC3lTKsj7bQVVq1sfB2HW2mZjpnPGm8WiWLQ8nSXbdrJ5TS/SNzf135QHcnhhRTp3zMnE7tPxR/Cubd9WmY8nPVnB9rQ5WH8R2YKxPTcWY3tuhxGRMzGWWYzEqON/gE0e0twO3D5lUnDYeWf1OhqxHcbX38mv787lwes8z0l2BgOHVtKnfw2LH+tHdN/2b9OPlouS95I4IJ/7nvyl2/nwkEoevG01T74yBqWO/wrA8lIbC2cN4MGFP6MU7NrUi9j4zmuDquV1+J9vQ6xNdYv+NBBrbwv1mU4KplZiP8mKre/xsWmcTuF34xIJDHbw6Kv76Z9YxYF0f16fH0thrg27j+Lepw/zq6m5vPNsjOcMezJ6DtZrGqYIhgCXAG+Juaq3g5wHfKqUqlRKlQKfe0qglFqslEp69bnoG/vFNVmOkbF15Ge73/bl59iI6mNaClZFYLCD0kIr+dl2ovrUuqStpSCn7VvG2P41xMTX8sKKdN78fhdRsXUs/GoXYVF1FOTYG2U05nXEPa+CHJ9GeRarIjDIQWmRlbY45YwKBp1WyZtrt/OXj9OJS6jh6SXpnpqG/KIAeoc3WaxR4ZXkFwW2iHfG0ExuvHwLs/42jrr6pnIE+NUy//epvPrJmfy4r7dHeQU5Pm7WZWvtaLRP87q3by98/+9Q7rtqKL+/eiiHf/Yjc79fu/GtURYcuU0WniPXiTWq9eFYtaIe/4vdy9hg/driLPicYaVud8u7iyKHT7Mx07njraLUytZ1vTjrfMPnbGGuHRDqai2kLgkncWRlu23QGq5t31aZjyt6DrbjKKXWYyzsjcJw9O1a1vavjGNjw0kJVqL71WCzO0mZUExaaohbhLTUEMZda8wJnnd5MVu/M3YUp6WGkDKhGLuPk+h+NcQl1JK+ufWHGwAZP/kz6bRTmTxqKJNHDSUv2860y4ZSlGcnfWsgfRKqG8sx9ooi0paHupdjeQgXTSwwynFZEVvXBdPe/pB//SOKG846jcmjhzPjmkQy9/syc5Ln2ZOf9kcR17uUmMgybFYHF5y9j3Wb3VcmnByfz/TJa5m1YBzFZU3THTarg7l3ryB17cms2dj6Q77mGHWvcal7IWnLw9zrviKUi67JN+teyNZ1Qe3WHSAkwlBSvYLrufymXJa+H9VufPspFuoPOanPcqLqFFXL6/E7r6USr8twoEoV9uFNQ9RZqlC1xlXtKHZSu82BLaHl5ba/Noi4hNpOHW8h4fUEBhvK3MfPyRljyjm017hkwns3/GkrzrmkhIz0jl9K6VsCPJb5uOHl9ICeImiGiAzB8IdQABwAhoqIL+CPsQniu3aSrwHeEJH5GHW8AnjJG7mWmN31998QzbPv7sNihdT3wzmw24+b789h91Z/0lJDWPpeODMXHOT1tT9SVmxl3l3GUqADu/1Y80Uoi1el43AICx+Kw+k0LvoHFh3gtORyQsLr+cfGXbz9TDTL3otosxxOh7Do4XieeHsPFqsidUkkB3b7c9P0LPZsDyBteShLl0Qy87n9vLZmB2XFVuZPa5pqeHPtdgKCHNjsxnzjrBsHcXDP0XntdzotLHgnmaf/sBSLRfH1t4PJyArjN1dtIj0jknVb+nPnrzbg71vHnN+tBOBIQS9mLxhHytn7OW1wDsG9arjk3D0APPnKGH4+5KHuj8TzxFvpRh98EMmBPf7cND2TPdsCSFsRxtIlUcx8dh+vrd5GWbHNve7fbW2q+8VFzLopkYN7/Lnr0YMkDDUstnf/1sejBSs2IWSGHwX3VhrLtC63Yx9opXRxDT5DrPiNMS6fquX1+I+z43qzVZ/hpPip6kbfc71u9sGe0PLuwonw/Kw45nXieAuPrmPG3w5isYDFAmu+COH7Fca89x8XHiQkoh4R+HmnHwv+2PpKk/ZwOlovc7fRg6cIRHXm48VjREQcwPaGn8BDSql/mWFPA1cD+4Fy4HOl1BuuW9ZEJANIUkrli8gsYDKQCxwE/qOU+ouI3AmglHqxrXIES7gaJd2zkU1s3fefV3PhyG6T7btyW7fJBoj91tdzpC7if/WVMSvUR5uO1vlKAwG9+6nEidM9RwS2vDD9mOV1lB5lwSql2pxAVErNBGa2cj7F5XiAy/ETwBOtxG9TsWo0mhOPE/2lhxqNRtMz6cYHWN6gFaxGozmx0QpWo9FoOp+GnVw9Fa1gNRrNCY04e66G1QpWo9GcuOg5WI1Go+k69BSBRqPRdBVawWo0Gk3XoC1YjUaj6Sq0gtVoNJouoPPeKtslaAXb05Duc3Dmu2q750hdhKpr3/l1V5P1i+6T/86htd0m+4Z+o7tNdmeg18FqNBpNV9KDHFY1RytYjUZzQqMtWI1Go+kK9EYDjUaj6Tr0Qy6NRqPpIrSC1Wg0mq5AoR9yaTQaTVehH3JpNBpNV6EVrEaj0XQ+eqOBRqPRdBVKaYfbJzJJKaXcOTcLq0Xx9XvhfLAw2i3c7uPk/gUHGTS8itIiG/Pu7M+Rwz4ATJp2hEuuK8ThFF6Y3YdNq41300//60FGXVRGcb6NOy5IbFP2mWNLuOvRg1isiqXvR/HBC7EtZM/46z4GDa+ktMjG/GknceSwL0Gh9cx+cS+DT6tg+UeRLHqkPwD+gQ7+8uGPjekjY+tY+WkELz0W77EdzhxTbJTFoli6JIoPXuzTsizP7GPQsApKi23Mn3YyRzJ9Of3cEm6ZeQibXVFfJ7wyP56t64Nb5Pn1u2Gd0rae+uuuuZmM/3Uhj9/WnzvnZuHj48TpgIpyKxYLvDYvlg0rgztV9h8XHmDQiCocdUL6Fn/+NrMfjnqhV0g90/96iFCLPwood9bgoOmR+NZvQnl7zkCcDki57ghXTs10q0veYV9ennEypQV2eoXWc9eC3UTEGlt+8zN9ePn+kynM9gWBmW/uIqpfjcd+9hZP7Xxc6bn6le7b+O6CiDwrIve5/F4mIq+4/H5GRFp9+bmITBGRPq2FHSsWi2LqvExm35DAbSmJnD+hmPhB1W5xxl9XSHmxjd+MPoVPXo7kt7OzAIgfVE3KhGJuPz+RWdcnMG1+JhaLMRJSl4Qz64YEz7LnHmD25EHcftEwUq4sIH5QlbvsSfmUl9i4ZexpfPpqNLc8cAiA2hrhrb/E8fIT/dziV1VYmXrZsMZPbqYPa5eGedcOjx1g9pTB3H7xcKMsJzcry6/yKC+xcsv5I/j01ZjGspQW2nj01sHcdelw/jJjIPf/9edW8+yMtvXUX4NOq6RXiANoivefb3th91U8NbU/8+/qz7T5hztd9spPwrj1vETuuGAwPn6KS68vAODX9+Ty805/ip1VlDurCbT4NJbV6YA3Zg9k5ls7eXrlZtZ/FsXh3f5u7fPu4wM495pcnly+havvO8SSJ/s3hr1432AuvzOTP3+zmblfbCU4ss5jP3uLN9fF8USUd5/uoEcoWGAtcA6AiFiASOBUl/BzgHVtpJ0CdEjBiohXlnvi6ZVkZfiQc9CX+joLqz4LJXl8iVuc5PElLP/QUFLffhnKyHPLAUXy+BJWfRZKXa2FI4d8ycrwIfH0SgB2fN+LsqL2i5A4soLsDF9yDvlRX2dh9RfhJI8rcpc9rogVH0casr8KZ+ToMkBRU2Vl58Yg6mra7t64hGpCI+rY8UMvz+0wopzsA65liWi/LF+HM/KcUkDx865ACnMNxXFgtz++fk7sPs4WeXZG27bXXxaL4raHs3j18VgsFhrjOR3C/p/8SR5fQmCwg8Ij9k6XvWFlMOZsIembA4iMNZRd/KBqtn5ntL8DhRULggDw85YgogdU07t/DTYfxS+uzGNTarhb+2TuCeDU0YaMoeeUNIYf3u2PwyEMH2OE+QU68fXvvMWi3lwXxw0FOJV3n26gpyjYdUCyeXwqsAMoE5EwEfEFTgEuFpENIrJDRBaLwUQgCXhHRLaIiL+InCkiq0Vkk2kJxwKIyCoReU5ENgL3elOoiJg68rKarIr8bHvjxdFAZEw9eVnGRel0CBWlVoLDHUTGNk/rQ0SM91ZEREwtedntp3ctn9MhVJRZCQ6r9yr/sVcUsPrLcDAv6PbLUkdetm9TWXJ8iIhx9z4VEd0Up62ynHtpEXt3BFJXa2mZZye0bXv9deVv8lmfGkJhrh2Exnj/eCaGAYOrue7eXOa+vZ/nZ8V1uuwGrDbFhROL2PhNEAD7d/kz+jJDMdmwYEGwmP1RmONDRJ+mNg6PraUox9ctv/hTKtjwdQQAG5eGU11uo6zIRs4+fwKC63n2tiE8dMkI3n18AE4HnYY3dT2uKC8/3UCPULBKqSygXkTiMazV9cD3GEo3CdgOLFRKnaWUGgb4A5crpT4CNgI3KKVGAvXA34GJSqkzgdeAJ1xE+SilkpRSzzQvg4jcLiIbRWRjHZ03V9VTGXtlIas+izhu8voPquSWPx5iwawBx01mA+HRdZx3RTGfvRbZIizlqmJ2/BDA8g/CePimBGb+/SDSRfeTd88/zI60wMa7hiULe9MrxEGoxR8/sVNPx6zMG2Zn8GNaCA9dMoIf00IIi6nBYlE4HEL6D8HcMHs/c7/cSu5BX9Z82LsrqtQj6KwpAhG5RETSRWSviDzQSvh0EdklIttE5N8i0r+1fFzpEQrWZB2Gcm1QsOtdfq8FzheR70VkO3AB7lMIDSQCw4DlIrIFmA30dQlf0pZwpdRiU/km2TEshYIcO1EuVkRkbB352Xa3dPk5NqL6GP/eFqsiMNhBaaGV/OzmaWspyHFP2x4FOT5Exbaf3rV8FqsiMMhBqYepB4CEUyqxWhV7dwR6WRY7UbFNfzqRMbUU5Pi4xznSFKd5WSJjann4pT385Q8DyT7o13qendC2bfXXycOq6DOgltfX/cib3+/C7qM4/ypjiuOS6wrIzfQhP9vOj5sC8fFVBIfXd5rsBm6YnkNIRD0vzWmazaost/LM7+ONOVhVgwXBaSrZ8JhaClysxMJsH8Ji3P/4w2Jq+f3LPzFv6VZ+NfMAAIEhDsJja+g/tILe/Wuw2uDM8YXs3+55KshbvLkujifiVF592s1DxAo8D1wKDAWuE5GhzaJtBpKUUqcBHwFPeypbT1KwDfOwwzGmCNIwLNiG+ddFGJbpcOBlwK+VPATYqZQaaX6GK6Uudgmv6EiB0rcEEJdQS3S/Gmx2JykTiklLDXGLk5YawrhrjYv1vMuLzTk1IS01hJQJxdh9nET3qyEuoZb0zQHey94aSJ+EmkbZY68oJG25+wOptBWhXHRNviH7skK2rgvCm1v+lCsLWPW599Zr+rZe9BlQQ3TfhrIUkLYitFlZwprKcmmhuVJACAyq57HX0nn9qX7s2hTUZp6d0bZt9dcP/w7mupGnMnnUUCaPGkpNlVCUZye6Xw15WXbOv9qI1+/kanx8nZQU2DpNNsAl1xeQlFLG/N/1R6mm/gkMdmCzGwrVV2zUKUfjnezAEWXkZPiTe9CX+loh7fMozhxX6NY+ZYU2nKbR+/nCvqRMygXgpBHlVJbaKC0w/uB2rQ0hblCl1/3tCW+ui+OGt9MDni3Ys4G9Sql9Sqla4H1ggpsopb5RSjU0ZBruxlur9KRlWuuAGcA+pZQDKBSRUAxL9TYzTr6I9AImYvyDAJQBDVduOhAlIslKqfUiYgcGK6V2Hk2BnA7h+VlxzHt3HxYrpL4fzoHdftx8fw67t/qTlhrC0vfCmbngIK+v/ZGyYivz7jLuGg7s9mPNF6EsXpWOwyEsfCgOp9O4uB5YdIDTkssJCa/nHxt38fYz0Sx7L6KF7EWPxPPEW+mG7A8iObDHn5umZ7JnWwBpK8JYuiSKmc/u47XV2ygrtjF/2sDG9G9+t5WAIAc2uyL54iJm3ZTIwT3GU+gxlxfx8JRBHWqHRY/254m3fsJigdQPoziwJ4Cbfn+YPdsDXcryM699s5WyEhvz7z4JgCsnH6FP/xquvyeL6+8xnsQ/dHMiJQV29zzfC+uUtm2tv1rS1K8+fgql4IHnDxAaVceXb0UC0qmy73nyMEcO+/DcF3sAWPtVCO88G0P8oGpmPHeQUEsADpyUO5uexFttMGXuPp668VScDhg7KZe+iVV89Jd4Ek4r58yLC9m1PoQlT/ZHBIaMKmXK4+YKDStcP3s/8349DKUgYXg5F1x/xOv+9mY8eNfOXY+x0cDrKZ1I8xlMA4uVUovN4zjgkEvYYWBUO3n9FvjaY/lUD3GUYJroRcACpdRs89wbQLJSKlFEHgeuA3KA3cABpdQcEbkGmAdUYVi8icACIATjD+Q5pdTLIrIKmKGU2ogHgiVcjZILO7uKXiF2H8+RugqLZ+u3q1A1//3z3m3xv/rKmBXqo01KqaRjySM4uK9KOmuaV3G/Wflgm/LMB+aXKKVuNX/fBIxSSrXIXERuBKYBY5VS7Q7cHmPBmlZrcLNzU1yOZ2PMqTZP9zHwscupLcCYVuKldFJRNRpND6IDFmx7ZAKuC8f7mufcZYlcBMzCC+UKPWsOVqPRaDpG583BbgAGiUiCiPgAvwY+d40gIqcDLwFXKqVyvSlej7FgNRqNpuN0ji8CpVS9iEwDlgFW4DWl1E4ReQzYqJT6HPgz0Av4UEQADiqlrmwvX61gNRrNiU0nPUdSSn0FfNXs3CMuxxd1NE+tYDUazYmL0q+M0Wg0mq6jh6yEag2tYDUazYlNz9WvWsFqNJoTG3H23DkCrWA1Gs2Ji4IO+sg5rmgFq9FoTlgE1VkbDboErWA1Gs2JjVawGm+x+HeP0wyAfiu6z2nygXO6dyg6Rg/vNtk39PMcR9MOWsFqNBpNF6DnYDUajabr0KsINBqNpktQeopAo9FougSFVrAajUbTZfTcGQKtYDUazYmNXger0Wg0XYVWsBqNRtMFKAWOnjtHoBWsRqM5sdEWrEaj0XQRWsH2fJw5gy8B/gZY75vmz/rn3cPtPk7uX3CQQcOrKC2yMe/O/hw5bLxie9K0I1xyXSEOp/DC7D5sWm28HDcppZQ752ZhtSi+fi+cDxZGAzDy3DJufTgbi0VRVWHhmfviycrw5Y45mYw4by8Afv5OwqNqKMj1xWJRLPsohg9fdt9TabM7mfFUOiefWk5ZsZ3504eQm+nH4OFl3P3YHgBE4J2F8axfEQnAhJsyGX9tDiKw9MMYPnsrzqv2qVzvoPCZOnBCrwlWQifb3cIL/1pL1SbjVk1Vg6NI0X+lPwD1OU7yn6jDcUSBQO9nfbD3af99m2eOLeGuOYewWGHp+5F8sCimRX/MeDaDQcMrKS2yMn/qQI4c9iUotJ7ZL/7M4BGVLP8wgkWPxAPg6+dk1gs/E9u/BqdTSFsRwutP9vVY76QRh/ndzT9gsSi+/mYQSz4/zS18+JAc7rr5BwbGF/HEgrF8+8OAxrBbr9vI2acfBuCdT0awOi2hdRltjBPXunZk7EX1qeX+vx0kNKoeFHz1jwj++WqUW57X3JHL7Y9mc+2wUykt7Lga8FTm44YCOuGdXF1Fl71VVkQGiMiOZufmiMiMdtKkiMiXHvKdIiILO1iWDBGJbCvcmTPYCjwPXAoMnXiVL/GDqt3ijL+ukPJiG78ZfQqfvBzJb2dnARA/qJqUCcXcfn4is65PYNr8TCwWhcWimDovk9k3JHBbSiLnTyhuzPPu+Yd5amo8vxuXyDefhnHdvUcAeGlOHHdffQZ3X30GX7wTg1LCI7edyp2Xn8nYX+bR76QK9zJNzKG81Mat48/i0zf7cMsf9gNwYE8A9048nbuvPoOHbxvG3X/ai8Wq6D+ogvHX5vD7X41k6lVncHZKIbHxVR7bTzkUhU/XEf03H+KW+FKxzEHtPvd5r/DpPsS940fcO34E/8pG4PnWxrC8OXWE3Ggj7gM/Yl/3xRou7cqzWBRTHz/I7MmDuP3CoaRcWUj8IPdyjp+UT3mJlVvGDOPTV6K55UHjDcu1NcJbz8Tx8hMtledHi2O47YJhTL30FE5NqiAppaT9coiTu3/zPQ89NY5bZ1zF+efsJz6u2C1Obn4gf37xXFauHeh2/uzTD3FyQgF3PnAl9zz8S669fAcB/rWtymhrnDTWtYNjz1EvLH6sD7enDOHeywdxxZR8tzyj+tRyxtgyjhx2/5P0lvbG9vFHgXJ69+kG9Gu7Dc4G9lpidu+zxOyu/fizGpLHu198yeNLWP5hGADffhnKyHPLAUXy+BJWfRZKXa2FI4d8ycrwIfH0ShJPryQrw4ecg77U11lY9VloY54KISDIAUBgkIPCIy0H+viJR8jM8CfnsD/1dRbWfBVF8oWFbnF+cWEBK/5pWA7fLYtiRHIxoKiptuJ0GErMx8fZeAfVb2Al6duCGsN3bAhh9Lh8j41Ts9OJra9gj7MgdiHwYiuVaxxtxq9IdRB4saFga/c5wQH+o4zflgDB4te+gk0cWUF2hl9j263+Iozki90VW/LFJaz4KAKAb78KY+ToUqPuVVZ2buhFXbW7jJpqC9vWBwFQX2dh744AImNbKjy3cpycT1ZOEDm5QdQ7rKxan8A5SQfd4hzJD2L/wfAWd6n940rY/mMMTqeF6ho7+w6GkzQis3UZbYyTxrp2cOwV5trZuz0AgKoKK4f2+hEZ2+TI5445Wbz6eJ+jvrNub2wfdxTGQy5vPt1AtyhYEVklIk+JyA8isltEzmslztkisl5ENovIOhFJdAnuZ+axR0QedUlzo5nnFhF5SUSszfNtgzjgUMOPrGyn24AEiIypJy/LUIROh1BRaiU43EFkbB15WT6N8fKzfYiIqSMipvl5e2Oez/2hL4+/vZ9/bNzFhROLWLKwt5us3n2qiYypYd9PgU3pc3yIiK5xixfRu5a8bN/GMlWW2QgOrQcg8bRSXvhiE4s+38TCOSfjdAgH9gQyLKmUoNA6fP0cJI0tJDLWPc/WcOSBLbpJYdl6C4681q/O+mwn9VlO/JKMoVV3UGHpBbkza8i6sZrCBXUoR/tXttF2TX86+dk+RETXNYtT29i+TodQUWYlOKxtpe9KYHA9oy4qZsva4HbjRYZVklfg0gcFgUSGVXolY9+BMM4akYmvTz3BQdWMHJpN74iKFvEiwyrbHCeNcTo49lyJ7lvLScOq+Ok/hsJNHl9Cfo6dfbv8vapHa7Q3trsFpbz7dAPdacHalFJnA/cBj7YS/hNwnlLqdOARYJ5L2NnANcBpwLUikiQipwCTgNFKqZGAA7jB28Js3FqdKCIbRWSjg64dLFffns/smxK4MWkoqUvCuX1Ollv4mMvy+Glr0DG9ayh9WzB3XXEm9117Or+6/RB2HyeH9gXw4ct9efzVHcx9eQf7fgxstHQ7i4pUBwEXWBGrma8Dqrc4CbvXTuwbvtRnOin/0jtF2BVYrIoH/r6fz17vTc5B3y6Ts2l7HD9sieNvf/oXD929ml17euNwdm5be8IvwMHDr2Tw4iN9qCy34uvv5Nd35/LWn2M8Jz6R+B9VsG3VqOH8J+b3JmBAK/FCgA/NedxngVNdwpYrpQqUUlVmPucCFwJnAhtEZIv5eyDekZk0wq9SKZWklErqF+tLfrb7bXt+jo2oPobitVgVgcEOSgut5GfbierTdKsZGVtLQY6dgpzm5+vIz7YTEl7PwKFVpG82LKPVn4cyNMndshl7WR7rV0S6WZeRMbUUHHFXCAW5PkSZcSxWRUBQPaXF7g8sDu0LoLrSyoDBhozUj2O495rTmXnTCMpL7WRmeLZkrFFQf6SpO+tzFdao1pVFxXIHgeObbhysvQWfwRZjesEmBIy1Upve/u2a0XZNf3KRsbUUNJtGKcjxaWxfi1URGOSgtMjzDcu9Tx4gK8OXf77q+aFMflEAUS5WZ2REBflFAR7TNfDuP0dw54MTeGDeeEQUmdkhrctoZZy4xeng2AOw2hQPv5LByk/CWPt1KACx/WuIia/lhRXpvPn9LqJi63h+2W7CojpmULQ1trsHL5Xrf6GCLQDCmp0LBxom/Rq0h4PWVzPMBb5RSg0DrgBcPVE3by0FCPCmUmqk+UlUSs3xsqwbgEHOnMEJzpzBPtdM8CUt1f1iSEsNYdy1RQCcd3kxW7/rBQhpqSGkTCjG7uMkul8NcQm1pG8OIH1LAHEJtUT3q8Fmd5IyoZi01BDKSqwEBjuIG2hU/4wxZRza01S1vgmV9AqpJ/XjaPq5LF3QAAAP3UlEQVT0ryY6rhqb3cmYy/JIWxnuVqbvV0Zw0VXGA7Jzx+exLS0UEKLjqrFYjSbq3aeavgOrOHLYkBESblwYUbHVnDMun1Vfuk9PtIbvUAv1hxR1mU5UnTKs1PNaKrPaDCeOMvAdbnFJKzjLFI4iozzVG53YE9ofdulbA+mTUN3YdmOvKCJteahbnLTlIVw0sQCA8y4rYuu6YIwh0DaTZ2QSGOTgxTneebhO/zmSuJhSYqLKsFkdpCTvZ/0m79JaxElQL+PBT0J8IQnxRWzc1qd1Ga2ME1c6OvZAMf2ZQxza48cni5tWD2T85M+k005l8qihTB41lLxsO1PHD6Yor2PKsa2x3S0owOn07tMNdNkyLaVUuYhki8gFSqmVIhIONCyF+o0XWYQADU8FpjQLG2fmVwVcBdwCVAKficizSqlcMzxIKXXAkyBLzO56Z87gacAywPrpFzUc2O3HzffnsHurP2mpISx9L5yZCw7y+tofKSu2Mu+u/gAc2O3Hmi9CWbwqHYdDWPhQHE7zVvD5WXHMe3cfFiukvh/Ogd2GkntuRj8efjkD5YSyEit/nd500Y79ZR6r/xWF02Hhhbkn8firO7BYFKkfR3NwbyA33p3Bnh1BfP9NBMs+imHG0+m8smwDZSU2npo+BIBTzyzh2tsOU18vKCcs+tNJlBYbF9GsBT8SHFpHfb2FRY+dREWZ5yEgNiH8fjtH7qk1lmldYcXnJAtFL9Xhe4qFgDGGsq1IdRA4zopIk6ITqxB+r52cqTWgwGeIhaCr2rc0nQ5h0cPxPPH2HixWReqSSA7s9uem6Vns2R5A2vJQli6JZOZz+3ltzQ7Kiq3Mn9Z0s/Lm2u0EBDmw2RXJ44uZdeMgKsusXHdPDgf3+LHwqx8B+OLN3ix9v83FJTidFha+8QvmP7jcWCq36mQOHA5j8sTN7N4fwfpN8QwemM+c6SvpFVjLL844zM3XbuG2+6/CanPy7KNfA1BZZeep58/D6Wz5x+J0WlodJ8cy9k49u5yLri1i3y4/Fi1PB+D1+bFsWNn+nLO3OB3S5tjuFnrwOlhRXVg4ERmKsfypwZL9s1LqHRFZBcxQSm00l09tVEoNEJEU8/zlIpIMvAlUAP8CbjTjTMFQqiFAX+AfSqk/mfImAQ9iWOZ1wFSlVJqIZABJSql8EfkKuFUp5T7x6UKwhKtRcmHnNoaXWIM75yI4Grr3lTHd+JCE7n1ljGX15m6T3Z2sUB9tUkolHUseIfYodU7oNV7FXZr/0jHL6yhdutFAKbULOL+V8ykux/mYc7BKqVXAKvN4PTDYJdls8/wbwBttyFsCLGnl/ACX48s6UgeNRtODUaC6aY2rN+idXBqN5sSmB+/k0gpWo9Gc2PTgOVitYDUazYmLUt22QsAbtILVaDQnNtqC1Wg0mq5AoRzdtzPQE1rBajSaE5ce7q5QK1iNRnNi04OXaWl3hRqN5oRFAcqpvPp4QkQuEZF0EdkrIg+0Eu4rIkvM8O9FZICnPLWC1Wg0Jy6qcxxum65NG53uA9eZO1Fd+S1QpJQ6GcMB1VOeiqcVrEajOaFRDodXHw+cDexVSu1TStUC7wMTmsWZgLF9H+Aj4EJxdbzRCl3qi+BERUTyAI9OYtohkiavYccbLVvLPlFk91dKRXmO1jYistQshzf4Aa7vtlmslFps5jMRuEQpdav5+yZglFJqmousHWacw+bvn804bbaBfsjVCp3Q6RuPt1MJLVvL/l+S3YBS6pLulO8JPUWg0Wg0hmtUV2e/fWlyl9oijojYMDz6FbSXqVawGo1GYzrdF5EEEfEBfg183izO58Bk83gisFJ5mGPVUwRdw2ItW8vWsk8clFL1ItLodB94TSm1U0Qew/BX/TnwKvC2iOwFCjGUcLvoh1wajUbTRegpAo1Go+kitILVaDSaLkIrWC8QEYeIbBGRrSLyHxE55xjze8Ncd9da2DciMr7ZuftE5IVjkemS10gR8fq1OSJS3hlym+U5R0RmeBGvod13iMgXIhLqKY2X8tus09H0tYi80rDrR0QyzPfMNY/Tbp1F5FkRuc/l9zIRecXl9zMiMr2NtFNEpOUra1uPO8Bcz9mRsqWIyJce8p0iIgu9KYNLmlbb6r8JrWC9o8p8FfgIjJcqzvc2obmcoyO8R8vJ81+b5zuDkcCJ8l6yhnYfhvFQYepxlOl1XyulbjXfP3csrAXOARARC8bi+VNdws8B1rWRdgrglYJt4CjGpeYo0Aq24wQDRQBi8GfTwtpuvtW24R//WxH5HNhlxltoOpJYAfRuJ/+PgF+aS0UwHUr0AfxFZL1pVX0oIr3M8MtE5CcR2SQiCxosDREJFJHXROQHEdksIhPMPB8DJplW2qSjaQARucJ0drFZRFaISLR5fo4pc5WI7BORe1zSzBKR3SLyHZB4FGLXA3FmXiNFJE1EtonIpyISZp6/TUQ2mNbnxyISYJ5PMNtuu4g83gGZrn3tZsWZ/TnFPF4lIi0W3HewzuuAZPP4VGAHUCYiYSLiC5wCXGzWb4eILDbH1UQgCXjH7FN/ETlTRFabY2KZiMQ2lBN4BDgJuLeV8q4SkafMMbNbRM5rJc7ZZltuFpF1IuJar35mHntE5FGXNDeaeW4RkZfE2Pf/v4FSSn88fAAHsAX4CSgBzjTPXwMsx1jWEQ0cBGKBFIzXjSeY8f7PJV4foBiY2I68L4EJ5vEDwCvAGiDQPPdHjAvFDzjkIuc94EvzeB7Gq84BQoHdQCCGtbOwA3Uvb+VcGE0rUG4FnjGP52AoCl8MC6wAsANnAtuBAAyltRfj9exeyTbb7UOMbYoA24Cx5vFjwHPmcYRL2seBu83jz4GbzeOprdXJi75OaWhb8/dCYIp5vArjtfAAGWbdO1xnYD8QD9wB3AnMxbjbGA18C4S7xH0buKIV+XazD6LM35Mwlhw1xHsb2NFM7hxghhne0JeXASua192si808vgj42DyeAmQDEYA/xh9EEsYfwxeA3Yy3yKUvMoDI7r6+u/KjbxO8o0opNRJARJKBt0RkGHAu8J5SygEcEZHVwFlAKfCDUmq/mX6MS7wsEVnpQV7DNMFn5venwFXAWjF8S/hgWHRDgH0uct4DbjePLwaudJlb88O4eDuDvsAS0zLywVAMDfxLKVUD1IhILsYfz3nAp0qpSgDTsvcGfxHZgmG5/ggsF5EQIFQptdqM8yaG8gUYZlqooUAvjDWNYCioa8zjt2nfC1Jbfd1RjqbO6zCmAs4B/opR73MwFP1a4HwRmYmhtMOBnRjKy5VEYBhGW4Hx55TtEv4FcHorshvWa35ifm8CBrQSLwR4U0QGmWnsLmHLlVIFACLyCcb1UY/xZ7PBLI8/kNtG/f/r0Aq2gyil1osxMe/JX0HFMYj5DHhWRM7AuJj+gzF4r3ONJCIj28lDgGuUUunN0ow6hnI18Hfgr0qpz0UkBcMCaqDG5djBsY2xKqXUSPNWfxmG9flmO/HfAK5SSm01b99TXMI6vOC7WV/X4z6l5tfR/LygYR52OIYFeAj4A8Yf9uvAyxiW6iERmdNGGQTYqZRKbiUMDGUb1uxcOE1/kg3911bfzQW+UUpdbU5frXIJa97GyizPm0qpB9soz381eg62g4jIEAyroADjtm2SiFhFJArDUv2hlWRrXOLFAue3J0MpVQ58A7yGYZWmAaNF5GSzDIEiMhhIBwZKk+Nf1znVZcDdYpoNItJgtZQBQR2qdEtCaNqnPbm9iCZrgKvM+cEg4IqOCDOtwHswlE0FUOQyP3gT0GDNBgHZImIHbnDJYi1NDw5dz7dLs74+AAwVw+lyKHChh+RHU+d1wOVAoVLKoZQqxLDGk2l6wJUvxvy76yoU1z5NB6JM6xsRsYuI68OyKow2usAMDwcuAb7zonzg3vdTmoWNE5FwEfHHvOMC/g1MFJHeDfJEpL+Xsk54tIL1Dn9zgn4LsASYbN7uf4oxH7gVWAnMVErltJL+U2APsAt4C+P2HgAReUxErmwlzXvACIyphTyMwfyeiGwz0w9RSlUBvwOWisgmjAutxEw/F+P2bZuI7DR/g6G4h5r1mSQiSeKyHKgVAkTksMtnOobF+qEp06O7OqXUfzDabSvwNca+7w6hlNqM0dbXYSj1P5ttMRJjHhbgYeB7jAv7J5fk9wJTRWQ75oOydmi1r5VSh4APMCzLD4DNHsrbZp1F5E4RubOVZNsx5m/Tmp0rUYZLvJdN+ctwb8M3gBfNMlsxlO9TIrIVYz65+VKzm4GHzfgrgT8ppX5urz4uPA3MF5HNtLRwfwA+xuinj5VSG5WxumI2kGr213KM5xRuiMhX4uVSsxMJvVX2BEdEeimlyk1L9Xlgj1Lq2e4ul0aj0RbsfwO3mZbITozbt5e6uTwajcZEW7AajUbTRWgLVqPRaLoIrWA1Go2mi9AKVqPRaLoIrWA1R4W4e7r60NwMcLR5NXoXExfPVG3ETZGj8GYmbXu58ujRSTroUUy89Bam+e9HK1jN0eLq6aoWY+98I3KU3pqUZ89UKbRc16nR9Ei0gtV0Bt8CJ0tLL2JWMbyNbRDD89Ud0OiFrFXvYuLimUpELhHDe9hWEfm3uWPtTuD3pvV8nohEieE5a4P5GW2mjRCRVBHZaW6kEE+VEJF/iuGBaqeI3N4s7Fnz/L/NXXuIyEkistRM862580ujaUT7ItAcE6aleimw1Dx1BjBMKbXfVFIlSqmzxHC5t1ZEUjGcjSQCQzGcwezC2Bbsmm8Uxs6lMWZe4UqpQhF5EcMb1l/MeO8CzyqlvhOReIxdTqcAjwLfKaUeE5FfAr/1ojq3mDL8MZyTfGw6LwnEePHd70XkETPvaRgv/btTKbVHDB8Pi4ALjqIZNf+laAWrOVoaPF2BYcG+inHr7upF7GLgNGl6e0MIMAjvvIv9AljTkJe5L781LsLY+tvwO9jcqz8Gw00kSql/iUiRF3W6R0SuNo/7mWUtAJwY214B/gF8Yso4B2PLcEN6Xy9kaP6H0ApWc7Q0uvVrwFQ0rl7EBMMn67Jm8TrzjQoW4BdKqepWyuI1YngFuwhIVkpViuGcui2PWcqUW9y8DTQaV/QcrKYrWQbcJYZ3K0RksIgE4p13sTRgjIgkmGnDzfPNvYGlAnc3/JAmF45rgOvNc5fS0kVfc0KAIlO5DsGwoBuw0OS96nqMqYdSYL+IXGvKEBEZ4UGG5n8MrWA1XckrGPOr/xHjRXsvYdw1teldrAHTg9jtGLfjW2m6Rf8CuLrhIReGG8Mk8yHaLppWM/wJQ0HvxJgqOOihrEsBm4j8CDyJu0erCuBssw4X0OS96wbgt2b5dgITvGgTzf8Q2heBRqPRdBHagtVoNJouQitYjUaj6SK0gtVoNJouQitYjUaj6SK0gtVoNJouQitYjUaj6SK0gtVoNJou4v8B3txqcaCPMQAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_classes = 7\n",
    "\n",
    "predlist = torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "lbllist = torch.zeros(0,dtype=torch.long, device='cpu')\n",
    "all_masks_one_hot = torch.tensor([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, masks) in enumerate(val_loader):\n",
    "        model.eval()\n",
    "        inputs = inputs.to(device)\n",
    "        masks = masks.to(device)\n",
    "        outputs = model(inputs)\n",
    "        masks_one_hot = torch.tensor([])\n",
    "        for every in masks:\n",
    "            masks_one_hot = torch.cat((masks_one_hot, rgb_to_onehot(every.cpu().numpy()).view(1,num_classes, image_size, image_size)))\n",
    "        all_masks_one_hot=torch.cat([all_masks_one_hot,masks_one_hot.cpu()])\n",
    "        # Append batch prediction results\n",
    "        predlist=torch.cat([predlist,torch.argmax(outputs,dim=1).view(-1).cpu()])\n",
    "        lbllist=torch.cat([lbllist,torch.argmax(masks_one_hot,dim=1).view(-1).cpu()])\n",
    "predlist = predlist.numpy()\n",
    "lbllist = lbllist.numpy()\n",
    "\n",
    "# Confusion matrix, classification report and AUC\n",
    "conf_mat=confusion_matrix(lbllist, predlist)\n",
    "conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "target_names = ['Bord.','Veget.','Land','Road','Build.','Water','Unlabel.',]\n",
    "ConfusionMatrixDisplay(conf_mat, display_labels=target_names).plot()\n",
    "print(classification_report(lbllist, predlist, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "source": [
    "### Considering the size of the dataset I think that results that we got are pretty good!\n",
    "### Next we can plot results of our model's work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "aSKxE-oRDVGY"
   },
   "outputs": [],
   "source": [
    "masks = torch.tensor([])\n",
    "modeled_masks = torch.tensor([])\n",
    "rgb_modeled_masks = []\n",
    "with torch.no_grad():\n",
    "    for inputs, mask in val_loader:\n",
    "        model.eval()       \n",
    "        inputs= inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        masks = torch.cat((masks,mask.permute(0,3,1,2)))\n",
    "        modeled_masks = torch.cat((modeled_masks,outputs.cpu()))\n",
    "for every in modeled_masks:\n",
    "     rgb_modeled_masks.append(onehot_to_rgb(every.permute(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "VpQrK8lVg9ou"
   },
   "outputs": [],
   "source": [
    "normalize_transforms = transforms.Compose([\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "sqYpHTa-Vko0"
   },
   "outputs": [],
   "source": [
    "unnormalized_pics = torch.tensor([])\n",
    "with torch.no_grad():\n",
    "    for inputs, mask in val_loader:\n",
    "        unnormalized_pics = torch.cat((unnormalized_pics, inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fc4O-Rexu2RD",
    "outputId": "572c13e2-6fcc-4f3b-d694-57ec7cf0290f"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for j in range(16):\n",
    "    count += 1\n",
    "    fig = plt.figure(figsize=(20,8))\n",
    "\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax1.imshow(unnormalized_pics[j].permute(1,2,0))\n",
    "    ax1.set_title('Input Image', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax1.grid(False)\n",
    "\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax2.set_title('Ground Truth Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax2.imshow(masks[j].int().permute(1,2,0))\n",
    "    ax2.grid(False)\n",
    "\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    ax3.set_title('Predicted Mask', fontdict={'fontsize': 16, 'fontweight': 'medium'})\n",
    "    ax3.imshow(rgb_modeled_masks[j])\n",
    "    ax3.grid(False)\n",
    "\n",
    "#     plt.savefig('/content/drive/MyDrive/predictions/prediction_{}.png'.format(count), facecolor= 'w', transparent= False, bbox_inches= 'tight', dpi= 200)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQTeg7Uxu2Mu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgML86vau2LP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HduhhEm_u2JI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VUJMyoxu2Hv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7o-G2IV5u2FY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rvi-sZWWu2Dm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dN5DE5Dou2Bl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSbeyllXu1_a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em3iGSz_u19W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIx4d5ALu18E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x11KT2hru157"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NOgjqOquURq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HC0JAvxBu2Mh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Sw0Z4jvu4xY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "aerial_semantic_segmentation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18db1510dc21478abad9bd583e2ded1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f183a0445b264176a8940514bf4e7d9f",
       "IPY_MODEL_5b687de4133e45709d2c0d5a68a6b33a"
      ],
      "layout": "IPY_MODEL_d2d54ab1f8c64025b28ce3f533b078d8"
     }
    },
    "32611ed1c5ce4c30929bb5566e86fae6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b687de4133e45709d2c0d5a68a6b33a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32611ed1c5ce4c30929bb5566e86fae6",
      "placeholder": "",
      "style": "IPY_MODEL_770872b820d24bc5992beff5d0745704",
      "value": " 170M/170M [00:02&lt;00:00, 67.9MB/s]"
     }
    },
    "770872b820d24bc5992beff5d0745704": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "95696ed9c6ba4ee09ae774a9fd63236d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8013a5a38714a3a8eca27a7f182977a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d2d54ab1f8c64025b28ce3f533b078d8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f183a0445b264176a8940514bf4e7d9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_95696ed9c6ba4ee09ae774a9fd63236d",
      "max": 178728960,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a8013a5a38714a3a8eca27a7f182977a",
      "value": 178728960
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
